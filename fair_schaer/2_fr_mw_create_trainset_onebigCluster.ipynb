{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f6bc79-7ab8-4310-80f6-66580f75a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "DATABASE = \"longeval-web\"\n",
    "USER = \"dis18\"\n",
    "HOST = \"db\"\n",
    "PORT = \"5432\"\n",
    "PASSWORD = \"dis182425\"\n",
    "\n",
    "engine = create_engine(f\"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
    "\n",
    "df = pd.read_sql('select * from \"Topic\" limit 1', con=engine)\n",
    "sql_query = lambda x: pd.read_sql(x, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79ed796-b949-4b24-8be7-d6ae4bc0984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_connection():\n",
    "    \"\"\"\n",
    "    Creates an engine the process can use for multi processing.\n",
    "    Remark: Connection gets lost if each worker connects via the same connection.\n",
    "    \"\"\"\n",
    "    DATABASE = \"longeval-web\"\n",
    "    USER = \"dis18\"\n",
    "    HOST = \"db\"\n",
    "    PORT = \"5432\"\n",
    "    PASSWORD = \"dis182425\"\n",
    "    \n",
    "    engine = create_engine(f\"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
    "    \n",
    "    return lambda x: pd.read_sql(x, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b707cce2-3d4c-4523-b5c2-ebfbe348381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: ClaudeAI (ran into max connection problem)\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.pool import QueuePool\n",
    "def create_engine_with_pool():\n",
    "    \"\"\"\n",
    "    Creates an engine with proper connection pooling configuration.\n",
    "    \"\"\"\n",
    "    DATABASE = \"longeval-web\"\n",
    "    USER = \"dis18\"\n",
    "    HOST = \"db\"\n",
    "    PORT = \"5432\"\n",
    "    PASSWORD = \"dis182425\"\n",
    "    \n",
    "    return create_engine(\n",
    "        f\"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\",\n",
    "        poolclass=QueuePool,\n",
    "        pool_size=5,  # Number of permanent connections\n",
    "        max_overflow=10,  # Number of additional connections that can be created\n",
    "        pool_timeout=30,  # Timeout waiting for a connection (seconds)\n",
    "        pool_recycle=1800,  # Recycle connections after 30 minutes\n",
    "        pool_pre_ping=True  # Verify connection validity before using\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f8c7f7-d4de-4a0b-a4fd-2105241f3a37",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sub_collection    count\n",
      "0        2022-06  1775681\n",
      "1        2022-07  1777616\n",
      "2        2022-08  1787018\n",
      "3        2022-09  1210186\n",
      "4        2022-10  2418103\n",
      "5        2022-11  2433787\n",
      "6        2022-12  2534242\n",
      "7        2023-01  2537565\n",
      "8        2023-02  2526382\n"
     ]
    }
   ],
   "source": [
    "# Get sub_collection and count(*) for each\n",
    "query= \"\"\"\n",
    "select sub_collection, count(*)\n",
    "from \"Document\"\n",
    "group by sub_collection\n",
    "\"\"\"\n",
    "df_subcol_count = sql_query(query)\n",
    "print(df_subcol_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99a82b13-8a4e-4acd-81d9-117df218d210",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06 1775681 \n",
      "\n",
      "2022-07 1777616 \n",
      "\n",
      "2022-08 1787018 \n",
      "\n",
      "2022-09 1210186 \n",
      "\n",
      "2022-10 2418103 \n",
      "\n",
      "2022-11 2433787 \n",
      "\n",
      "2022-12 2534242 \n",
      "\n",
      "2023-01 2537565 \n",
      "\n",
      "2023-02 2526382 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "1    None\n",
       "2    None\n",
       "3    None\n",
       "4    None\n",
       "5    None\n",
       "6    None\n",
       "7    None\n",
       "8    None\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_col_name: str= None\n",
    "sub_count: int = None \n",
    "sub_batch_size = 1000\n",
    "\n",
    "df_subcol_count.apply(lambda x:print(x[\"sub_collection\"],x[\"count\"],\"\\n\"), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a0d4ce-0a0e-41ee-818f-3d2a72bf18df",
   "metadata": {},
   "source": [
    "# 2.Pipeline Term Frequency on Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fb7bb0-55ff-4cf6-a5bb-4b4af5ba7ae7",
   "metadata": {},
   "source": [
    "## 2.1 Inner Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ee66b5b-4fcd-4186-bd6e-b9b002ce058e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m \u001b[38;5;66;03m# memory efficiency\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munidecode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unidecode\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SnowballStemmer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc # memory efficiency\n",
    "import re\n",
    "import nltk\n",
    "from unidecode import unidecode\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('stopwords')\n",
    "french_stopwords = set(stopwords.words('french'))\n",
    "    \n",
    "# Add additional French stopwords (articles, prepositions, etc.)\n",
    "additional_stopwords = {\n",
    "    'a', 'au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'en',\n",
    "    'et', 'il', 'ils', 'je', 'j', 'la', 'le', 'les', 'leur', 'lui', 'ma',\n",
    "    'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ni', 'notre', 'nous', 'on',\n",
    "    'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'si', 'son',\n",
    "    'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre',\n",
    "    'vous',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "    # Numbers written as words\n",
    "    'zero', 'un', 'deux', 'trois', 'quatre', 'cinq', 'six', 'sept', 'huit', 'neuf', 'dix',\n",
    "        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10'\n",
    "}\n",
    "\n",
    "# Combine standard and additional stopwords\n",
    "stop_words = french_stopwords.union(additional_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0117929f-0e5a-4d66-855a-e2a9140030be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_terms(df_batch):\n",
    "    \"\"\"\n",
    "    Splits text of each document and stems the content.\n",
    "    Returns list of stemmed terms.\n",
    "    \"\"\"\n",
    "    fr_sbst = SnowballStemmer(\"french\")\n",
    "    french_stopwords = set(stopwords.words('french'))\n",
    "    \n",
    "    # Add additional French stopwords (articles, prepositions, etc.)\n",
    "    additional_stopwords = {\n",
    "        'a', 'au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'en',\n",
    "        'et', 'il', 'ils', 'je', 'j', 'la', 'le', 'les', 'leur', 'lui', 'ma',\n",
    "        'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ni', 'notre', 'nous', 'on',\n",
    "        'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'si', 'son',\n",
    "        'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre',\n",
    "        'vous',\n",
    "        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "        # Numbers written as words\n",
    "        'zero', 'un', 'deux', 'trois', 'quatre', 'cinq', 'six', 'sept', 'huit', 'neuf', 'dix',\n",
    "        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10'\n",
    "    }\n",
    "    \n",
    "    # Combine standard and additional stopwords\n",
    "    stop_words = french_stopwords.union(additional_stopwords)\n",
    "    # Create bag if words\n",
    "    # re replaces all punctuations\n",
    "    df_batch[\"text_fr_cleaned\"] = df_batch[\"text_fr\"].apply(lambda x: re.sub(r'[^\\w\\s]|[\\d]', '', x) if x else \"\")\n",
    "    words_document = [unidecode(word.lower()) for query in df_batch[\"text_fr_cleaned\"] for word in list(query.split())] # Attention!!! List Comprehension is always from left to right. (I forgot that again...)\n",
    "    \n",
    "    # 50 häufigsten Wörter nach Stemming\n",
    "    words_document_stem = [fr_sbst.stem(word) for word in words_document if not word.lower() in stop_words]\n",
    "\n",
    "    return words_document_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a17dc-709b-45f0-90fb-d443438481ca",
   "metadata": {},
   "source": [
    "## 2.2 Outer Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c855ada-f067-40b2-9607-046fec6ca453",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_batch_BoW(batch_id: int, batch, doc_identifier):\n",
    "    \"\"\"\n",
    "    Processes Single Batch from Parallel Batch Processing.\n",
    "    \"\"\"\n",
    "    batch_list = \"\".join(f\"'{batch[i]}',\" if i+1 <  len(batch) else f\"'{batch[i]}'\" for i in range(len(batch)))\n",
    "    #print(n_begin, n_end)\n",
    "    # Don#t need the extended query (as in Multi Hot Encoding) because i only need the docid info where evaluated docs are.\n",
    "    q_batch = f\"\"\"\n",
    "    select     distinct a.docid, a.text_fr\n",
    "    from       \"Document\" a\n",
    "    where      a.docid in ({batch_list}) \n",
    "    group by   a.docid, a.text_fr\n",
    "    \"\"\"\n",
    "    #print(q_batch)\n",
    "    engine = create_engine_with_pool()\n",
    "    try:\n",
    "        df_batch = pd.read_sql(q_batch, con=engine)\n",
    "        \n",
    "        gc.collect()\n",
    "        return batch_id, get_document_terms(df_batch)\n",
    "    finally:\n",
    "        engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "622e4095-b58a-463a-96bd-10e337944965",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parallel Processing\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "def parallel_bagofwords(first_doc, n_docs):\n",
    "    \"\"\"\n",
    "    Starts Parallel Processing.\n",
    "    \"\"\"\n",
    "    q_docs = f\"\"\"\n",
    "    select distinct(a.docid)\n",
    "    from \"Document\" a\n",
    "    join (\n",
    "          select ('doc'|| b_inner.docid)new_docid , *\n",
    "          from \"Qrel\" b_inner\n",
    "          where queryid in (\n",
    "                select queryid\n",
    "                from \"Qrel\" \n",
    "                where     sub_collection in ('2022-06', '2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12', '2023-01', '2023-02')\n",
    "                      and relevance <> '0' -- so that a bias is created in favour of relevant documents\n",
    "          )\n",
    "    ) b\n",
    "    on        a.docid = b.new_docid\n",
    "          and a.sub_collection = b.sub_collection\n",
    "    join (\n",
    "          select *\n",
    "          from \"Topic\"\n",
    "    ) c\n",
    "    on b.queryid = c.queryid  \n",
    "    where     b.sub_collection in ('2022-06', '2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12', '2023-01', '2023-02')\n",
    "          and b.relevance is not null\n",
    "          and a.sub_collection is not null\n",
    "          and b.queryid is not null\n",
    "    limit {int(n_docs)}\n",
    "    \"\"\"\n",
    "    sql_query = sql_connection()\n",
    "    docs_list = sql_query(q_docs)[\"docid\"].tolist()\n",
    "    doc_identifier = first_doc[:5]\n",
    "    doc_id = first_doc[5:]\n",
    "    #n_begin = int(doc_id)\n",
    "    func_slice_batch = lambda begin, end: docs_list[begin:end]\n",
    "\n",
    "    workers = 24  # max sind 32 ich nehm 12 # habe max worker für sql erreicht 12 sind zu viel\n",
    "    batch_size = 200\n",
    "\n",
    "    n_docs = n_docs if n_docs == len(docs_list) else len(docs_list)\n",
    "    batches = []\n",
    "    for i in range(0, n_docs, batch_size):\n",
    "        end_idx = min(i + batch_size, n_docs)\n",
    "        batches.append((i, func_slice_batch(i, end_idx)))\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=workers) as executor:\n",
    "        # Submit all batches and store futures\n",
    "        future_to_batch = {\n",
    "        executor.submit(process_batch_BoW, batch_id, batch, doc_identifier): batch_id for batch_id, batch in batches\n",
    "        }\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        with tqdm(as_completed(future_to_batch), total=len(future_to_batch), desc=\"Batch Progress\") as pbar:\n",
    "            for i, future in enumerate(pbar):\n",
    "                batch_id, result = future.result()\n",
    "                results[batch_id] = result\n",
    "                pbar.set_description(f\"Processing batch {i}\")\n",
    "\n",
    "    #print(\"Available batch keys:\", results.keys())\n",
    "    #print(\"Trying to access batches 0 to\", len(batches)-1)\n",
    "\n",
    "    final_results = []\n",
    "    for i in batches:\n",
    "        #print(i)\n",
    "        final_results.extend(results[i[0]])\n",
    "\n",
    "    print(f\"Processed {n_docs} items in {len(batches)} batches\")\n",
    "    print(f\"Time taken: {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"First few results: {final_results[:5]}\")\n",
    "\n",
    "    gc.collect()\n",
    "    return final_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "679037ac-1051-4166-b6fe-213a94dd7b1e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_numbers(text):\n",
    "    return int(re.sub(r'\\D', '', text))\n",
    "\n",
    "def top_n_words(n_docs:int):\n",
    "    \"\"\"\n",
    "    Starts transforming\n",
    "    \"\"\"\n",
    "    q_first_doc = f\"\"\"\n",
    "    select *\n",
    "    from \"Document\"\n",
    "    limit 1\n",
    "    \"\"\"\n",
    "    n_docs = int(n_docs)\n",
    "    first_doc = sql_query(q_first_doc)[\"docid\"].item()\n",
    "\n",
    "    # Get Words from Documents\n",
    "    words_document_stem = parallel_bagofwords(first_doc, n_docs)\n",
    "    # Create unique set\n",
    "    bag_words_stem = set(sorted(words_document_stem))\n",
    "    # Get Term Frequency\n",
    "    df_terms_stem = pd.DataFrame(bag_words_stem, columns=[\"term\"])\n",
    "    df_terms_stem [\"count\"] = 0\n",
    "    # Count Words and match with Data Frame\n",
    "    word_counts = Counter(words_document_stem)\n",
    "    df_terms_stem['count'] += df_terms_stem['term'].map(word_counts).fillna(0)\n",
    "    df_terms_stem = df_terms_stem.sort_values(by=[\"count\"], ascending=False)\n",
    "    \n",
    "\n",
    "    # Summary\n",
    "    print(f\"Top {len(df_terms_stem)} Words for all SubCollection:\") \n",
    "    print(\"Bag of Words\")\n",
    "    print(df_terms_stem)\n",
    "    print(f\"\\nTop Words:\")\n",
    "    print(df_terms_stem[\"term\"].tolist()[:10])    \n",
    "\n",
    "    return df_terms_stem\n",
    "    \n",
    "   #for n_begin in range(doc_start, doc_start+n_docs+1, batch_size):\n",
    "   #    n_end =  n_begin + batch_size\n",
    "   #    print(n_begin, n_end)\n",
    "   #    q_batch = f\"\"\"\n",
    "   #    select *\n",
    "   #    from \"Document\"\n",
    "   #            and docid between 'doc0{n_begin}' and 'doc0{n_end}'\n",
    "   #    \"\"\"\n",
    "   #    \n",
    "   #    df_batch = sql_query(q_batch)\n",
    "   #    print(df_batch)\n",
    "   #    \n",
    "   #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d9a6f-73e8-4a5b-bd61-91c2542f06b1",
   "metadata": {},
   "source": [
    "# 3. MultiHotEncoding Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271317af-7bcd-4697-b29f-6d24cc26e7bf",
   "metadata": {},
   "source": [
    "## 3.1 Inner Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a66ea2dd-2587-43d5-824b-67edc52091c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text (text, top_words_list):\n",
    "    \"\"\"\n",
    "    Stems text and returns only terms included in top_words_list.\n",
    "    \"\"\"\n",
    "    fr_sbst = SnowballStemmer(\"french\")\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    # Start\n",
    "    text = re.sub(r'[^\\w\\s]|[\\d]', '', text)\n",
    "    words_document = [unidecode(word.lower()) for word in list(text.split())]\n",
    "    words_document_stem = [fr_sbst.stem(word) for word in words_document if not word.lower() in stop_words]\n",
    "    return [word for word in words_document_stem if word in top_words_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2934bcbc-e77e-4c69-80ca-89b2029942ab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def  create_words_index(df_terms_stem, n_words):\n",
    "    \"\"\"\n",
    "    Creates translation table for term <> index.\n",
    "    \"\"\"\n",
    "    df_words_index = df_terms_stem.iloc[:n_words, :1].copy().reset_index()\n",
    "    df_words_index[\"index\"] = df_words_index.index.tolist()\n",
    "    df_words_index.to_csv(\"words_index_translationtable.csv\", index=False)\n",
    "    return df_words_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "238a3529-1d18-4d3e-ab7a-9171e7a7d85e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_term_index(word_list, term_idx_table):\n",
    "    \"\"\"\n",
    "    Searches for the corresponding index for each term.\n",
    "    \"\"\"\n",
    "    df_index = pd.DataFrame(word_list, columns=[\"term\"])\n",
    "    df_index = df_index.merge(term_idx_table, how=\"left\", left_on=\"term\", right_on=\"term\")\n",
    "    return df_index[\"index\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c0a4767-54f6-4ee1-aadf-46ee0a71362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topiccluster(docs_topwords):\n",
    "    \"\"\"\n",
    "    Enriches Data Frame with corresponding Topic Cluster.\n",
    "    \"\"\"\n",
    "    topic_cluster = pd.read_csv(f\"topics_cluster_all_subcollections.csv\")\n",
    "    #print(topic_cluster.columns.tolist())\n",
    "    #print(topic_cluster.head())\n",
    "    # Merge Cluster to Document Data Frame\n",
    "    docs_topwords = docs_topwords.merge(topic_cluster[[\"queryid\", \"cluster\"]], how=\"left\", left_on=\"queryid\", right_on=\"queryid\")\n",
    "    \n",
    "    return docs_topwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a94c9579-70e6-482a-8a19-f7edb0f5443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def documents_topwords_filter(df_batch, top_words_list):\n",
    "    \"\"\"\n",
    "    Splits text of each document and stems the content.\n",
    "    Filters stemmed terms, to only include top_words_list entries.\n",
    "    Returns DataFrame with Filtered terms for each document.\n",
    "    \"\"\"\n",
    "    #df_batch_filtered = pd.DataFrame(columns=[\"docid\",\"term_list_stemmed\"])\n",
    "    #df_batch_filtered[\"docid\"] = df_batch[\"docid\"].copy()\n",
    "    #df_batch_filtered[\"term_list_stemmed\"] = df_batch[\"text_fr\"].apply(lambda x: stem_text(x, top_words_list))\n",
    "    df_batch[\"term_list_stemmed\"] = df_batch[\"text_fr\"].apply(lambda x: stem_text(x, top_words_list))\n",
    "    \n",
    "    return df_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee5c130d-fb09-4e92-8ab7-e4fd2d82ec96",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_batch_MHE(batch_id: int, batch, doc_identifier, top_words_list, df_words_index):\n",
    "    \"\"\"\n",
    "    Processes Single Batch from Parallel Batch Processing.\n",
    "    \"\"\"\n",
    "    batch_list = \"\".join(f\"'{batch[i]}',\" if i+1 <  len(batch) else f\"'{batch[i]}'\" for i in range(len(batch)))\n",
    "    #print(n_begin, n_end)\n",
    "    q_batch = f\"\"\"\n",
    "    select a.docid, a.text_fr, a.sub_collection, c.queryid, cast(AVG(cast(b.relevance as int)) as NUMERIC(5,2)) as relevance\n",
    "    from \"Document\" a\n",
    "    join (\n",
    "          select ('doc'|| b_inner.docid)new_docid , *\n",
    "          from \"Qrel\" b_inner\n",
    "          where queryid in (\n",
    "                select queryid\n",
    "                from \"Qrel\" \n",
    "                where sub_collection in ('2022-06', '2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12', '2023-01', '2023-02')\n",
    "          )\n",
    "    ) b\n",
    "    on        a.docid = b.new_docid\n",
    "          and a.sub_collection = b.sub_collection\n",
    "    join (\n",
    "          select *\n",
    "          from \"Topic\"\n",
    "    ) c\n",
    "    on b.queryid = c.queryid  \n",
    "    where     b.sub_collection in ('2022-06', '2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12', '2023-01', '2023-02')\n",
    "          and b.relevance is not null\n",
    "          and a.sub_collection is not null\n",
    "          and b.queryid is not null\n",
    "          and a.docid in ({batch_list})\n",
    "    group by a.docid, a.text_fr, a.sub_collection, c.queryid\n",
    "    \"\"\"\n",
    "    engine = create_engine_with_pool()\n",
    "    try:\n",
    "        df_batch = pd.read_sql(q_batch, con=engine)\n",
    "        df_batch = documents_topwords_filter(df_batch, top_words_list)\n",
    "        df_batch[\"term_idx\"] = df_batch[\"term_list_stemmed\"].apply(lambda x: get_term_index(word_list=x,term_idx_table=df_words_index))\n",
    "\n",
    "        gc.collect()\n",
    "        return batch_id, df_batch\n",
    "    finally:\n",
    "        engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f731e1d9-568a-4b0b-b674-7797bb9d8712",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 3.2 Outer Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70d6e9b7-2130-463e-8200-c13bafd63069",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parallel Processing\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "def parallel_MultiHotEncoding(first_doc, n_docs, top_words_list, df_words_index):\n",
    "    \"\"\"\n",
    "    Starts Parallel Processing.\n",
    "    \"\"\"\n",
    "    q_docs = f\"\"\"\n",
    "    select distinct a.docid\n",
    "    from \"Document\" a\n",
    "    join (\n",
    "          select ('doc'|| b_inner.docid)new_docid , *\n",
    "          from \"Qrel\" b_inner\n",
    "          where queryid in (\n",
    "                select queryid\n",
    "                from \"Qrel\" \n",
    "                where sub_collection in ('2022-06', '2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12', '2023-01', '2023-02')\n",
    "          )\n",
    "    ) b\n",
    "    on        a.docid = b.new_docid\n",
    "          and a.sub_collection = b.sub_collection\n",
    "    join (\n",
    "          select *\n",
    "          from \"Topic\"\n",
    "    ) c\n",
    "    on b.queryid = c.queryid  \n",
    "    where     b.sub_collection in ('2022-06', '2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12', '2023-01', '2023-02')\n",
    "          and b.relevance is not null\n",
    "          and a.sub_collection is not null\n",
    "          and b.queryid is not null\n",
    "    --limit {int(n_docs)} -- no limits this time (legacy code still included for limit purpose...)\n",
    "    \"\"\"\n",
    "    \n",
    "    sql_query = sql_connection()\n",
    "    docs_list = sql_query(q_docs)[\"docid\"].tolist()\n",
    "    doc_identifier = first_doc[:5]\n",
    "    doc_id = first_doc[5:]\n",
    "    #n_begin = int(doc_id)\n",
    "    func_slice_batch = lambda begin, end: docs_list[begin:end]\n",
    "    \n",
    "    workers = 24  # max sind 32 ich nehm 12 # habe max worker für sql erreicht 12 sind zu viel\n",
    "    batch_size = 100\n",
    "\n",
    "    n_docs = n_docs if n_docs == len(docs_list) else len(docs_list)\n",
    "    batches = []\n",
    "    for i in range(0, n_docs, batch_size):\n",
    "        end_idx = min(i + batch_size, n_docs)\n",
    "        batches.append((i, func_slice_batch(i, end_idx)))\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=workers) as executor:\n",
    "        # Submit all batches and store futures\n",
    "        future_to_batch = {\n",
    "        executor.submit(process_batch_MHE, batch_id, batch, doc_identifier, top_words_list, df_words_index): batch_id for batch_id, batch in batches\n",
    "        }\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        with tqdm(as_completed(future_to_batch), total=len(future_to_batch), desc=\"Batch Progress\") as pbar:\n",
    "            for i, future in enumerate(pbar):\n",
    "                batch_id, result = future.result()\n",
    "                results[batch_id] = result\n",
    "                pbar.set_description(f\"Processing batch {i}\")\n",
    "\n",
    "    #print(\"Available batch keys:\", results.keys())\n",
    "    #print(\"Trying to access batches 0 to\", len(batches)-1)\n",
    "    #print(results)\n",
    "    first_batch = batches[0][0]\n",
    "    final_results = results[first_batch]\n",
    "    for i in batches[1:]:\n",
    "        #print(i)\n",
    "        final_results = pd.concat([final_results, results[i[0]]], ignore_index=True) # i is the batch tuple (batch_id, (func_inner, ... addittional parameters))\n",
    "\n",
    "    print(f\"Processed {n_docs} items in {len(batches)} batches\")\n",
    "    print(f\"Time taken: {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Filtered Text for {len(top_words_list)} Top Words of the Corpus.\")\n",
    "    print(f\"First few results:\")\n",
    "\n",
    "    gc.collect()\n",
    "    return final_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec3ce725-5466-4bc7-803c-8e2aa2340d26",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multi_hot_encoding(n_docs:int, df_terms_stem, n_words):\n",
    "    \"\"\"\n",
    "    Starts transforming\n",
    "    \"\"\"\n",
    "    # Init Variables\n",
    "    n_docs = int(n_docs)\n",
    "    q_first_doc = f\"\"\"\n",
    "    select *\n",
    "    from \"Document\"\n",
    "    limit 1\n",
    "    \"\"\"\n",
    "    first_doc = sql_query(q_first_doc)[\"docid\"].item()\n",
    "\n",
    "    # Preparation for Parallel Processing\n",
    "    top_words_list = df_terms_stem.iloc[:n_words, 0].to_list()\n",
    "    df_words_index = create_words_index(df_terms_stem, n_words=n_words)\n",
    "    \n",
    "    # Iterative Processing via Parallel Processing\n",
    "    docs_topwords = parallel_MultiHotEncoding(first_doc, n_docs, top_words_list, df_words_index)\n",
    "    #print(docs_topwords)  \n",
    "\n",
    "    # Mass Processing Via Data Frame Vectorization\n",
    "    docs_topwords = docs_topwords.astype({'queryid': 'int64'})\n",
    "    docs_topwords = get_topiccluster(docs_topwords)\n",
    "    print(docs_topwords)\n",
    "    # MARKER!!!!\n",
    "    # Need to add the cluster and relevance!\n",
    "    # Remark: need to see how i pass the relevance from the sql database. its a bit more complicated now.\n",
    "    # Remark: adjust parallel_bag of words as well.\n",
    " \n",
    "\n",
    "    return docs_topwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20013336-dacb-4b63-afa5-833f9bbee6a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "engine.dispose()\n",
    "#print(n_docs//10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23c1fb87-ea79-45a0-8d39-1dd0b1ed5f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512116\n"
     ]
    }
   ],
   "source": [
    "# Get sub_collection and count(*) for each\n",
    "search_collection = '2023-02'\n",
    "# !!! has to be changed \n",
    "query= f\"\"\"\n",
    "select count(*)\n",
    "from \"Document\" a\n",
    "    join (\n",
    "          select ('doc'|| b_inner.docid)new_docid , *\n",
    "          from \"Qrel\" b_inner\n",
    "          where queryid in (\n",
    "                select queryid\n",
    "                from \"Qrel\" \n",
    "                where sub_collection in ('2022-06', '2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12', '2023-01', '2023-02')\n",
    "          )\n",
    "    ) b\n",
    "    on        a.docid = b.new_docid\n",
    "          and a.sub_collection = b.sub_collection\n",
    "    join (\n",
    "          select *\n",
    "          from \"Topic\"\n",
    "    ) c\n",
    "    on b.queryid = c.queryid  \n",
    "    where     b.sub_collection in ('2022-06', '2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12', '2023-01', '2023-02')\n",
    "          and b.relevance is not null\n",
    "          and a.sub_collection is not null\n",
    "          and b.queryid is not null\n",
    "\"\"\"\n",
    "n_docs = sql_query(query).iloc[0,0].item() # [0,0] <- [first row, first column]\n",
    "print(n_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "608ca01e-055f-40e4-bc65-022ded4737e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training set for all Sub Collections in one.\n",
      "Found documents with relevance score:\t\t512116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 916: 100%|██████████| 917/917 [48:59<00:00,  3.21s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 91657 items in 917 batches\n",
      "Time taken: 2981.93 seconds\n",
      "Filtered Text for 10000 Top Words of the Corpus.\n",
      "First few results:\n",
      "           docid                                            text_fr  \\\n",
      "0       doc12272  Taxe d'aménagement : les espaces de stationnem...   \n",
      "1       doc12272  Taxe d'aménagement : les espaces de stationnem...   \n",
      "2       doc12367  Lycée Jean Jaurès - Argenteuil LE LYCEE JEAN J...   \n",
      "3       doc12367  Lycée Jean Jaurès - Argenteuil LE LYCEE JEAN J...   \n",
      "4       doc12367  Lycée Jean Jaurès - Argenteuil LE LYCEE JEAN J...   \n",
      "...          ...                                                ...   \n",
      "370603   doc9785  Les horaires d'ouverture du zoo | ZooParc de B...   \n",
      "370604   doc9785  Les horaires d'ouverture du zoo | ZooParc de B...   \n",
      "370605   doc9785  Les horaires d'ouverture du zoo | ZooParc de B...   \n",
      "370606   doc9785  Les horaires d'ouverture du zoo | ZooParc de B...   \n",
      "370607   doc9785  Les horaires d'ouverture du zoo | ZooParc de B...   \n",
      "\n",
      "       sub_collection  queryid  relevance  \\\n",
      "0             2022-06     2762       0.00   \n",
      "1             2022-08     2762       0.00   \n",
      "2             2022-08     3011       0.00   \n",
      "3             2022-10       98       0.00   \n",
      "4             2022-12        7       0.00   \n",
      "...               ...      ...        ...   \n",
      "370603        2022-12      240       1.33   \n",
      "370604        2022-12     3101       1.33   \n",
      "370605        2023-01     1617       1.00   \n",
      "370606        2023-01      240       1.00   \n",
      "370607        2023-01     3101       1.67   \n",
      "\n",
      "                                        term_list_stemmed  \\\n",
      "0       [tax, damenag, espac, station, mod, calcul, pe...   \n",
      "1       [tax, damenag, espac, station, mod, calcul, pe...   \n",
      "2       [lyce, jean, jaur, argenteuil, lyce, jean, jau...   \n",
      "3       [lyce, jean, jaur, argenteuil, lyce, jean, jau...   \n",
      "4       [lyce, jean, jaur, argenteuil, lyce, jean, jau...   \n",
      "...                                                   ...   \n",
      "370603  [horair, douvertur, zoo, zooparc, beauval, men...   \n",
      "370604  [horair, douvertur, zoo, zooparc, beauval, men...   \n",
      "370605  [horair, douvertur, zoo, zooparc, beauval, men...   \n",
      "370606  [horair, douvertur, zoo, zooparc, beauval, men...   \n",
      "370607  [horair, douvertur, zoo, zooparc, beauval, men...   \n",
      "\n",
      "                                                 term_idx  cluster  \n",
      "0       [174, 2659, 69, 739, 457, 285, 607, 1707, 39, ...       17  \n",
      "1       [174, 2659, 69, 739, 457, 285, 607, 1707, 39, ...       17  \n",
      "2       [1883, 788, 4321, 3836, 1883, 788, 4321, 436, ...       53  \n",
      "3       [1883, 788, 4321, 3836, 1883, 788, 4321, 436, ...       22  \n",
      "4       [1883, 788, 4321, 3836, 1883, 788, 4321, 436, ...       41  \n",
      "...                                                   ...      ...  \n",
      "370603  [364, 1176, 5513, 8494, 4450, 761, 354, 1009, ...        2  \n",
      "370604  [364, 1176, 5513, 8494, 4450, 761, 354, 1009, ...        2  \n",
      "370605  [364, 1176, 5513, 8494, 4450, 761, 354, 1009, ...       15  \n",
      "370606  [364, 1176, 5513, 8494, 4450, 761, 354, 1009, ...        2  \n",
      "370607  [364, 1176, 5513, 8494, 4450, 761, 354, 1009, ...        2  \n",
      "\n",
      "[370608 rows x 8 columns]\n",
      "\n",
      "!!!DONE!!!\n"
     ]
    }
   ],
   "source": [
    "# Start creating Training Set\n",
    "print(f\"Creating training set for all Sub Collections in one.\")\n",
    "print(f\"Found documents with relevance score:\\t\\t{n_docs}\")\n",
    "\n",
    "# Get Top Words for each subcollection\n",
    "df_terms_stem=top_n_words(n_docs)\n",
    "df_terms_stem.to_csv(f\"top_terms_stemmed_all_subcollections.csv\", index=False)\n",
    "gc.collect()\n",
    "#df_terms_stem = pd.read_csv(f\"top_terms_stemmed_all_subcollections.csv\")\n",
    "\n",
    "# Get seperate MultiHotEncoded and Docs DataFrame from subcollection \n",
    "docs_topwords = multi_hot_encoding(n_docs, df_terms_stem, n_words=10_000) #n_docs//10\n",
    "docs_topwords.to_csv(f\"bis2023-02_train_set_documents_top_terms_all_subcollections.csv\", index=False)\n",
    "gc.collect()\n",
    "    \n",
    "print(\"\\n!!!DONE!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85e95019-5a52-408e-9732-2240d1452687",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs_topwords_compressed = docs_topwords.loc[:,[\"docid\", \"term_idx\", \"cluster\", \"relevance\"]]\n",
    "docs_topwords_compressed.drop_duplicates(subset=[\"docid\",\"cluster\", \"relevance\"],inplace=True)\n",
    "docs_topwords_compressed.to_csv(f\"bis2023-02_train_set_documents_top_terms_all_subcollections_compressed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d517c721-5c68-4bf2-b8a3-b0c63c22ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_np(df, filename):\n",
    "    \"\"\"\n",
    "    Save entire DataFrame to NumPy format, preserving lists and data types.\n",
    "    \n",
    "    Parameters:\n",
    "        df: The pandas DataFrame to save\n",
    "        filename: Filename without extension\n",
    "    \"\"\"\n",
    "    # Store column types for reconstruction\n",
    "    column_types = {}\n",
    "    \n",
    "    # Convert DataFrame to a dictionary of columns\n",
    "    data_dict = {}\n",
    "    \n",
    "    # Process each column appropriately\n",
    "    for col in df.columns:\n",
    "        # Sample the column to detect type\n",
    "        sample = df[col].iloc[0] if len(df) > 0 else None\n",
    "        \n",
    "        if isinstance(sample, list):\n",
    "            # It's a list column\n",
    "            data_dict[col] = np.array(df[col].tolist(), dtype=object)\n",
    "            column_types[col] = 'list'\n",
    "        elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Already numeric, save directly\n",
    "            data_dict[col] = df[col].values\n",
    "            column_types[col] = 'numeric'\n",
    "        elif isinstance(sample, str):\n",
    "            # It's a string column, try to convert numeric strings\n",
    "            try:\n",
    "                # Check if all values can be converted to integers\n",
    "                df[col].astype(int)\n",
    "                data_dict[col] = df[col].astype(int).values\n",
    "                column_types[col] = 'int'\n",
    "            except (ValueError, TypeError):\n",
    "                try:\n",
    "                    # Check if all values can be converted to floats\n",
    "                    df[col].astype(float)\n",
    "                    data_dict[col] = df[col].astype(float).values\n",
    "                    column_types[col] = 'float'\n",
    "                except (ValueError, TypeError):\n",
    "                    # Regular string column\n",
    "                    data_dict[col] = df[col].values\n",
    "                    column_types[col] = 'string'\n",
    "        else:\n",
    "            # Other types, save as is\n",
    "            data_dict[col] = df[col].values\n",
    "            column_types[col] = 'other'\n",
    "    \n",
    "    # Add column types to the data dictionary\n",
    "    data_dict['__column_types__'] = np.array([column_types], dtype=object)\n",
    "    \n",
    "    # Save everything to a single .npz file\n",
    "    np.savez_compressed(f\"bis2023-02_{filename}.npz\", **data_dict)\n",
    "    print(f\"Saved DataFrame to {filename}.npz with type detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc50ed82-f876-4588-9432-89786e2b42a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DataFrame to bis2023-02_train_set_documents_top_terms_all_subcollections_compressed.npz with type detection\n"
     ]
    }
   ],
   "source": [
    "save_df_np(docs_topwords_compressed, \"bis2023-02_train_set_documents_top_terms_all_subcollections_compressed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
