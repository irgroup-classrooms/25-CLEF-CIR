{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05d62ae8-1493-4a63-9007-e9a58dd3545c",
   "metadata": {},
   "source": [
    "# LongEval\n",
    "In diesem Notebook findet ihr einen einfachen starter um ein Retrieval System für den LongEval Shared Task aufzusetzen.\n",
    "\n",
    "Weitere Informationen findet ihr hier:\n",
    "- [Shared Task Website](https://clef-longeval.github.io/)\n",
    "- [Overview Paper 2024](http://www.zubiaga.org/publications/files/alkhalifa2024longeval-extended.pdf)\n",
    "- [Overview Paper 2023](http://www.zubiaga.org/publications/files/alkhalifa2023longeval-overview.pdf)\n",
    "- [LongEval Test Collection Paper](https://www.semanticscholar.org/reader/f40debce2b7caf35ea0730c27c5330989d20b300)\n",
    "\n",
    "Alle Datensätze findet ihr unter `datasets/LongEval`. PyTerrier Indexe für jeden Zeitpunkt mit der üblichen Preprocessing Pipeline findet ihr in `datasets/LongEval/index`. In der Datei `datasets/LongEval/metadata.yml` findet ihr Metadaten die euch gegebenenfalls helfen die Sub-Collections zu organisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20cd710c-de74-4327-a986-a54e80113098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java started and loaded: pyterrier.java, pyterrier.terrier.java [version=5.11 (build: craig.macdonald 2025-01-13 21:29), helper_version=0.0.8]\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pyterrier as pt\n",
    "if not pt.java.started():\n",
    "    pt.java.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed12d69-49fa-4523-b6f7-40151ebe1c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "DATABASE = \"longeval-web\"\n",
    "USER = \"dis18\"\n",
    "HOST = \"db\"\n",
    "PORT = \"5432\"\n",
    "PASSWORD = \"dis182425\"\n",
    "\n",
    "engine = create_engine(f\"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
    "\n",
    "df = pd.read_sql('select * from \"Topic\" limit 1', con=engine)\n",
    "sql_query = lambda x: pd.read_sql(x, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba92fb09-eab2-40ac-a972-433ad7deab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"longeval-web\"\n",
    "language = \"fr\"\n",
    "sub_collection = \"2023-02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b08c716-3218-49b2-9a3b-6400f2929a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/home/jovyan/work/datasets/LongEval-Web\"\n",
    "\n",
    "with open(BASE_PATH + \"/metadata.yml\", \"r\") as yamlfile:\n",
    "    config = yaml.load(yamlfile, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8752e7b-2906-4ee8-8d51-275ffe61048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = os.path.join(\".\", BASE_PATH, f\"index/{dataset}-{language}-{sub_collection}-pyterrier\")\n",
    "topics_path = os.path.join(BASE_PATH, \"release_2025_p1/French/queries.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eaae643-2cfb-444d-b43f-46d7a665c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_topics = f\"\"\"\n",
    "select distinct b.queryid qid, b.text_fr query\n",
    "from \"Qrel\" a \n",
    "join  (\n",
    "        select  *\n",
    "        from    \"Topic\"\n",
    "      ) b\n",
    "      on      a.queryid = b.queryid\n",
    "join (\n",
    "        select distinct docid\n",
    "        from   \"Document\"\n",
    "        where  sub_collection = '{sub_collection}'\n",
    "      )c\n",
    "      on ('doc' || a.docid) = c.docid\n",
    "where a.sub_collection = '{sub_collection}' \n",
    "\"\"\"\n",
    "\n",
    "topics = sql_query(q_topics)\n",
    "#topics = pd.read_csv(topics_path, sep=\"\\t\", names=[\"qid\", \"query\"])\n",
    "topics[\"qid\"] = topics[\"qid\"].astype(str)\n",
    "\n",
    "topics[\"query\"] = topics[\"query\"].str.replace(\"'\", \"\")\n",
    "topics[\"query\"] = topics[\"query\"].str.replace(\"*\", \"\")\n",
    "topics[\"query\"] = topics[\"query\"].str.replace(\"/\", \"\")\n",
    "topics[\"query\"] = topics[\"query\"].str.replace(\":\", \"\")\n",
    "topics[\"query\"] = topics[\"query\"].str.replace(\"?\", \"\")\n",
    "topics[\"query\"] = topics[\"query\"].str.replace(\")\", \"\")\n",
    "topics[\"query\"] = topics[\"query\"].str.replace(\"(\", \"\")\n",
    "topics[\"query\"] = topics[\"query\"].str.replace(\"+\", \"\")\n",
    "spam = [\"59769\", \"6060\", \"75200\", \"74351\", \"67599\", \"74238\", \"74207\", \"75100\", \"58130\"]\n",
    "topics = topics[~topics[\"qid\"].isin(spam)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9cd518e-71f7-4234-8326-68f7f5df7f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:30:13.677 [main] WARN org.terrier.structures.BaseCompressingMetaIndex -- Structure meta reading data file directly from disk (SLOW) - try index.meta.data-source=fileinmem in the index properties file. 4.3 GiB of memory would be required.\n"
     ]
    }
   ],
   "source": [
    "index = pt.IndexFactory.of(index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7fc5bd-6d97-4c18-9f3b-88c6dbde15d2",
   "metadata": {},
   "source": [
    "### Run Erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "317a6223-8a2c-42d1-82b2-3c8671510b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25 = pt.terrier.Retriever(index, wmodel=\"BM25\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7def6892-557a-4700-af77-1ea45ceafce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loaded index with 2037717 documents.\n"
     ]
    }
   ],
   "source": [
    "print(\">>> Loaded index with\", index.getCollectionStatistics().getNumberOfDocuments(), \"documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77258d8b-cb51-4f03-93c2-2226f1c5a356",
   "metadata": {},
   "source": [
    "# Run Laden \n",
    "Erstellen eines Runs für alle Topics dauert sehr lange. Alternativ könnt ihr auch einen BM25 Baseline Run laden und eure Ansätze als Re-Ranking implementieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af735230-3ee6-46cd-9af5-a8a27d5a25ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read own run\n",
    "run = pd.read_csv(\"run_modelprediction_2023-02.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78a18f5c-7a44-48fa-8ee0-1240ad2f74fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_base = pt.io.read_results(f\"{BASE_PATH}/runs/{dataset}-{language}-{sub_collection}-BM25.gz\")\n",
    "run_base[\"docno\"] = run_base[\"docno\"].str.strip(\"doc\")  # the indexed documents prefix the docid with `doc`, this needs to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc6df762-059b-4a41-8b0f-246abebc6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_map = {f\"{qid}\":1 for qid in run[\"qid\"].drop_duplicates().tolist() }\n",
    "run_base[\"qid_bool\"] = run_base[\"qid\"].map(qid_map)\n",
    "run_base_small = run_base.dropna(subset=[\"qid_bool\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66825061-8407-442a-9af5-02ac572eac73",
   "metadata": {},
   "source": [
    "# System Evaluieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23b731a2-cdff-4379-8a8d-f5ef5b729ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3673,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_base_small[\"qid\"].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70dabe4c-6be3-4114-a51a-acf82bd56c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_qrels = f\"\"\"\n",
    "select a.queryid qid, a.docid docno, cast(a.relevance as int) label\n",
    "from \"Qrel\" a \n",
    "join  (\n",
    "        select  *\n",
    "        from    \"Topic\"\n",
    "      ) b\n",
    "      on      a.queryid = b.queryid\n",
    "join (\n",
    "        select distinct docid\n",
    "        from   \"Document\"\n",
    "        where  sub_collection = '{sub_collection}'\n",
    "      )c\n",
    "      on ('doc' || a.docid) = c.docid\n",
    "where a.sub_collection = '{sub_collection}' \n",
    "\"\"\"\n",
    "\n",
    "qrels = sql_query(q_qrels)\n",
    "#qrels = pt.io.read_qrels(BASE_PATH + f\"/release_2025_p1/French/LongEval Train Collection/qrels/{sub_collection}_{language}/qrels_processed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "618862f4-e9d3-45a8-86c6-8bdaaae693b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qid', 'docno', 'label']\n"
     ]
    }
   ],
   "source": [
    "print(qrels.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4b03a27-9c12-4c27-9587-8b346a93db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"docno\"] = run[\"docno\"].str.strip(\"doc\")  # the indexed documents prefix the docid with `doc`, this needs to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d603994-1c0d-4172-b41e-819c4c886fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.Experiment:  50%|█████     | 1/2 [00:07<00:07,  7.53s/system]/opt/conda/lib/python3.11/site-packages/pyterrier/model.py:228: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[column] = dataframe[column].astype(dtype)\n",
      "pt.Experiment: 100%|██████████| 2/2 [00:32<00:00, 16.30s/system]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>bpref</th>\n",
       "      <th>map</th>\n",
       "      <th>ndcg</th>\n",
       "      <th>ndcg_cut_10</th>\n",
       "      <th>P.10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qid    docno  rank      score      ...</td>\n",
       "      <td>0.247167</td>\n",
       "      <td>0.103080</td>\n",
       "      <td>0.150342</td>\n",
       "      <td>0.118722</td>\n",
       "      <td>0.027428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qid    docno  rank      score\\n0   ...</td>\n",
       "      <td>0.262908</td>\n",
       "      <td>0.130859</td>\n",
       "      <td>0.171742</td>\n",
       "      <td>0.144436</td>\n",
       "      <td>0.029921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name     bpref       map  \\\n",
       "0             qid    docno  rank      score      ...  0.247167  0.103080   \n",
       "1             qid    docno  rank      score\\n0   ...  0.262908  0.130859   \n",
       "\n",
       "       ndcg  ndcg_cut_10      P.10  \n",
       "0  0.150342     0.118722  0.027428  \n",
       "1  0.171742     0.144436  0.029921  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"qid\", \"docno\", \"rank\", \"score\"]\n",
    "run_eval = run[cols]\n",
    "pt.Experiment(\n",
    "    [run_base_small, run_eval],\n",
    "    topics,\n",
    "    qrels,\n",
    "    eval_metrics=[\"bpref\", \"map\", \"ndcg\", \"ndcg_cut_10\", \"P.10\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2b0f3c2-4924-4f6c-aab1-654506e9f6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           qid    docno  rank      score       name  qid_bool\n",
      "0            3  2214755     0  24.258737  pyterrier       1.0\n",
      "1            3   684186     1  23.376940  pyterrier       1.0\n",
      "2            3   637997     2  23.182743  pyterrier       1.0\n",
      "3            3   430968     3  23.010934  pyterrier       1.0\n",
      "4            3  3430721     4  22.815825  pyterrier       1.0\n",
      "...        ...      ...   ...        ...        ...       ...\n",
      "7735425  75397  3364209   995  11.714027  pyterrier       NaN\n",
      "7735426  75397  3245800   996  11.706049  pyterrier       NaN\n",
      "7735427  75397  1290080   997  11.703555  pyterrier       NaN\n",
      "7735428  75397  1690427   998  11.696082  pyterrier       NaN\n",
      "7735429  75397  2906989   999  11.694390  pyterrier       NaN\n",
      "\n",
      "[7735430 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(run_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "654a0912-d25f-4fc0-a41a-dcdd792d1a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.Experiment:  50%|█████     | 1/2 [00:07<00:07,  7.65s/system]/opt/conda/lib/python3.11/site-packages/pyterrier/model.py:228: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe[column] = dataframe[column].astype(dtype)\n",
      "pt.Experiment: 100%|██████████| 2/2 [00:31<00:00, 15.99s/system]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>qid</th>\n",
       "      <th>measure</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11181</th>\n",
       "      <td>qid    docno  rank      score\\n0   ...</td>\n",
       "      <td>100</td>\n",
       "      <td>ndcg</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11210</th>\n",
       "      <td>qid    docno  rank      score\\n0   ...</td>\n",
       "      <td>1000</td>\n",
       "      <td>ndcg</td>\n",
       "      <td>0.231378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9741</th>\n",
       "      <td>qid    docno  rank      score\\n0   ...</td>\n",
       "      <td>1006</td>\n",
       "      <td>ndcg</td>\n",
       "      <td>0.274785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8254</th>\n",
       "      <td>qid    docno  rank      score\\n0   ...</td>\n",
       "      <td>1007</td>\n",
       "      <td>ndcg</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11654</th>\n",
       "      <td>qid    docno  rank      score\\n0   ...</td>\n",
       "      <td>1009</td>\n",
       "      <td>ndcg</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>qid    docno  rank      score      ...</td>\n",
       "      <td>99</td>\n",
       "      <td>ndcg</td>\n",
       "      <td>0.194959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>qid    docno  rank      score      ...</td>\n",
       "      <td>990</td>\n",
       "      <td>ndcg</td>\n",
       "      <td>0.189200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>qid    docno  rank      score      ...</td>\n",
       "      <td>992</td>\n",
       "      <td>ndcg</td>\n",
       "      <td>0.356207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>qid    docno  rank      score      ...</td>\n",
       "      <td>996</td>\n",
       "      <td>ndcg</td>\n",
       "      <td>0.327395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7980</th>\n",
       "      <td>qid    docno  rank      score      ...</td>\n",
       "      <td>998</td>\n",
       "      <td>ndcg</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15962 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name   qid measure  \\\n",
       "11181             qid    docno  rank      score\\n0   ...   100    ndcg   \n",
       "11210             qid    docno  rank      score\\n0   ...  1000    ndcg   \n",
       "9741              qid    docno  rank      score\\n0   ...  1006    ndcg   \n",
       "8254              qid    docno  rank      score\\n0   ...  1007    ndcg   \n",
       "11654             qid    docno  rank      score\\n0   ...  1009    ndcg   \n",
       "...                                                  ...   ...     ...   \n",
       "42                qid    docno  rank      score      ...    99    ndcg   \n",
       "387               qid    docno  rank      score      ...   990    ndcg   \n",
       "388               qid    docno  rank      score      ...   992    ndcg   \n",
       "389               qid    docno  rank      score      ...   996    ndcg   \n",
       "7980              qid    docno  rank      score      ...   998    ndcg   \n",
       "\n",
       "          value  \n",
       "11181  0.000000  \n",
       "11210  0.231378  \n",
       "9741   0.274785  \n",
       "8254   1.000000  \n",
       "11654  0.000000  \n",
       "...         ...  \n",
       "42     0.194959  \n",
       "387    0.189200  \n",
       "388    0.356207  \n",
       "389    0.327395  \n",
       "7980   0.000000  \n",
       "\n",
       "[15962 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"qid\", \"docno\", \"rank\", \"score\"]\n",
    "run_eval = run[cols]\n",
    "df_res = pt.Experiment(\n",
    "    [run_base_small, run_eval],\n",
    "    topics,\n",
    "    qrels,\n",
    "    eval_metrics=[\"ndcg\"],#[\"bpref\", \"map\", \"ndcg\", \"ndcg_cut_10\", \"P.10\"],\n",
    "    verbose=True,\n",
    "    perquery=True,\n",
    ")\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5883ca80-416f-4ea4-a120-1c6660eba083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
