{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b89dd750-fc35-4765-b8df-808f4e3d49d2",
   "metadata": {},
   "source": [
    "# 1. MultiHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1225d14f-f9d1-4696-9a2b-a55611026943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 15:20:52.860407: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-23 15:20:52.864765: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-23 15:20:52.876368: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742743252.894726 3773044 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742743252.899767 3773044 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742743252.914175 3773044 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742743252.914192 3773044 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742743252.914194 3773044 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742743252.914196 3773044 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-23 15:20:52.919547: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast # lib for converting list saved as string back to list object\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "#from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80776ea3-83e9-4099-b522-0f0e1793c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Connection\n",
    "DATABASE = \"longeval\"\n",
    "USER = \"dis18\"\n",
    "HOST = \"db\"\n",
    "PORT = \"5432\"\n",
    "PASSWORD = \"dis182425\"\n",
    "\n",
    "#engine = create_engine(f\"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
    "\n",
    "#df = pd.read_sql('select * from \"Topic\" limit 1', con=engine)\n",
    "#sql_query = lambda x: pd.read_sql(x, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5ca7598-a309-48ae-aa54-3bb6d5af1465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = pd.read_csv(\"train_set_documents_top_terms_2022-06.csv\")\n",
    "#print(test_df)\n",
    "#print(test_df[\"cluster\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3008e405-d4bb-4762-be88-9af8c1ef18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def get_subsetdata(subset: str = None)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load DataFrame from NumPy format with lists preserved.\n",
    "    \n",
    "    Parameters:\n",
    "        filename: Filename without extension\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with all original data types preserved\n",
    "    \"\"\"\n",
    "    # Load data from NPZ file\n",
    "    data = np.load(f\"train_set_documents_top_terms_{subset}.npz\", allow_pickle=True)\n",
    "    \n",
    "    \n",
    "    # Create dictionary for DataFrame constructor\n",
    "    df_dict = {}\n",
    "    \n",
    "    # Track detected types for verification\n",
    "    detected_types = {}\n",
    "    saved_types = {}\n",
    "    \n",
    "    # Check if we have saved type information\n",
    "    has_type_info = '__column_types__' in data.files\n",
    "    if has_type_info:\n",
    "        saved_types = data['__column_types__'][0]\n",
    "        print(f\"Found saved type information for {len(saved_types)} columns\")\n",
    "    \n",
    "    # Process each column\n",
    "    for key in data.files:\n",
    "        if key == '__column_types__':\n",
    "            continue  # Skip metadata\n",
    "            \n",
    "        # Get the array\n",
    "        arr = data[key]\n",
    "        \n",
    "        # Detect the type regardless of saved info (for verification)\n",
    "        if arr.dtype == np.dtype('O'):  # Object dtype\n",
    "            # Sample non-None values\n",
    "            samples = [x for x in arr[:min(10, len(arr))] if x is not None]\n",
    "            \n",
    "            if len(samples) > 0:\n",
    "                sample = samples[0]\n",
    "                \n",
    "                # Check if it's likely a list\n",
    "                if isinstance(sample, np.ndarray) or (hasattr(sample, '__iter__') and not isinstance(sample, (str, bytes))):\n",
    "                    detected_types[key] = 'list'\n",
    "                    # Convert numpy arrays to Python lists\n",
    "                    df_dict[key] = [x.tolist() if isinstance(x, np.ndarray) else x for x in arr]\n",
    "                else:\n",
    "                    # It's some other object type\n",
    "                    detected_types[key] = 'object'\n",
    "                    df_dict[key] = arr\n",
    "            else:\n",
    "                # No valid samples, keep as is\n",
    "                detected_types[key] = 'object'\n",
    "                df_dict[key] = arr\n",
    "        elif np.issubdtype(arr.dtype, np.integer):\n",
    "            detected_types[key] = 'int'\n",
    "            df_dict[key] = arr\n",
    "        elif np.issubdtype(arr.dtype, np.floating):\n",
    "            detected_types[key] = 'float'\n",
    "            df_dict[key] = arr\n",
    "        else:\n",
    "            # Other numpy dtype\n",
    "            detected_types[key] = str(arr.dtype)\n",
    "            df_dict[key] = arr\n",
    "    \n",
    "    # Create initial DataFrame\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    \n",
    "    # Verify and apply additional type corrections\n",
    "    for col in df.columns:\n",
    "        # Apply saved type information but verify it's correct\n",
    "        if has_type_info and col in saved_types:\n",
    "            saved_type = saved_types[col]\n",
    "            detected_type = detected_types.get(col, 'unknown')\n",
    "            \n",
    "            if saved_type != detected_type:\n",
    "                print(f\"Warning: Column '{col}' has saved type '{saved_type}' but detected as '{detected_type}'\")\n",
    "                # Try to reconcile differences\n",
    "                \n",
    "                # Special case: strings that should be lists\n",
    "                if saved_type == 'list' and detected_type != 'list':\n",
    "                    # Check if strings that look like lists\n",
    "                    if isinstance(df[col].iloc[0], str) and df[col].iloc[0].startswith('['):\n",
    "                        try:\n",
    "                            df[col] = df[col].apply(ast.literal_eval)\n",
    "                            print(f\"  Fixed: Converted string representations to lists in '{col}'\")\n",
    "                        except (ValueError, SyntaxError):\n",
    "                            print(f\"  Failed: Could not convert strings to lists in '{col}'\")\n",
    "                \n",
    "                # Special case: strings that should be numeric\n",
    "                elif saved_type in ('int', 'float') and detected_type not in ('int', 'float'):\n",
    "                    try:\n",
    "                        if saved_type == 'int':\n",
    "                            df[col] = df[col].astype(int)\n",
    "                            print(f\"  Fixed: Converted to integers in '{col}'\")\n",
    "                        else:\n",
    "                            df[col] = df[col].astype(float)\n",
    "                            print(f\"  Fixed: Converted to floats in '{col}'\")\n",
    "                    except (ValueError, TypeError):\n",
    "                        print(f\"  Failed: Could not convert to {saved_type} in '{col}'\")\n",
    "        \n",
    "        # For all columns, check for string representations that need conversion\n",
    "        if isinstance(df[col].iloc[0], str):\n",
    "            # Check for string representation of lists\n",
    "            if df[col].iloc[0].startswith('[') and df[col].iloc[0].endswith(']'):\n",
    "                try:\n",
    "                    df[col] = df[col].apply(ast.literal_eval)\n",
    "                    print(f\"Converted string representations to lists in column '{col}'\")\n",
    "                except (ValueError, SyntaxError):\n",
    "                    pass  # Not valid list representations\n",
    "            \n",
    "            # Check for numeric strings not already converted\n",
    "            elif col not in saved_types or saved_types[col] not in ('int', 'float'):\n",
    "                # Try converting to integer\n",
    "                try:\n",
    "                    df[col] = df[col].astype(int)\n",
    "                    print(f\"Converted strings to integers in column '{col}'\")\n",
    "                except (ValueError, TypeError):\n",
    "                    # Try converting to float\n",
    "                    try:\n",
    "                        df[col] = df[col].astype(float)\n",
    "                        print(f\"Converted strings to floats in column '{col}'\")\n",
    "                    except (ValueError, TypeError):\n",
    "                        pass  # Not numeric strings\n",
    "    df.loc[df[\"relevance\"]>0, [\"relevance\"]]=1\n",
    "    return df[[\"docid\", \"term_idx\", \"relevance\", \"cluster\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59d1e1de-f125-4fb2-9302-6719294387a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train, Val, Test\n",
    "# Source: Help with ClaudeAI\n",
    "def train_val_test_split(\n",
    "    df: pd.DataFrame,\n",
    "    train_size: float = 0.7,\n",
    "    val_size: float = 0.15,\n",
    "    test_size: float = 0.15,) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\" \n",
    "    Returns sliced Data Frames for Train, Val, Test.\n",
    "    \"\"\"\n",
    "    if not np.isclose(train_size + val_size + test_size, 1.0):\n",
    "        raise ValueError(\"Split proportions must sum to 1\")\n",
    "\n",
    "    # Calculate sizes for each split\n",
    "    n_samples = len(df)\n",
    "    n_train = int(n_samples * train_size)\n",
    "    n_val = int(n_samples * val_size)\n",
    "    \n",
    "    # Create random indices for splitting\n",
    "    indices = np.random.permutation(n_samples) # -> So cool!!! I need to remember that!!!\n",
    "        \n",
    "    # Simple random split\n",
    "    train_idx = indices[:n_train]\n",
    "    val_idx = indices[n_train:n_train + n_val]\n",
    "    test_idx = indices[n_train + n_val:]\n",
    "\n",
    "     # Create the splits\n",
    "    train_df = df.iloc[train_idx].copy().reset_index(drop=True)\n",
    "    val_df = df.iloc[val_idx].copy().reset_index(drop=True)\n",
    "    test_df = df.iloc[test_idx].copy().reset_index(drop=True)\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac2a5cf3-3028-499a-a682-6528b93c9965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the term idx into a multihotencoded tensor\n",
    "# Source: DIS21a.1 Heisenberg\n",
    "# Remark: Normal OneHotEncoding is not possible because its in fact a multi Hot Encoding.\n",
    "\n",
    "# just take the first 10.000 most frequent words\n",
    "def vectorize_sequences(sequences, dimension=10_000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences): # i is the n_th review whereas the sequence assigns like a list of fields (like pandas data frame) all the right fields results[i, [3, 5]] = 1\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc269ce3-fd2e-48b4-b642-d256784968b7",
   "metadata": {},
   "source": [
    "# 2. Train Subset Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9c89d62-fba2-4fb9-94dd-f056a0147667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources\n",
    "# https://medium.com/@kevinnjagi83/building-deep-learning-models-with-multi-output-architectures-61d1c3c81d40\n",
    "# https://medium.com/@sthanikamsanthosh1994/custom-models-with-tensorflow-part-1-multi-output-model-c01a78e67d47\n",
    "# ClaudeAI (altered, because no one explained how to prepare labels...)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def get_label(df):\n",
    "    \"\"\"\n",
    "    Returns multilabel varaiable for Dense Layer Training.\n",
    "    \"\"\"\n",
    "    cluster_labels = np.array(df[\"cluster\"].tolist())\n",
    "    relevance_labels = np.array(df[\"relevance\"].tolist())\n",
    "    \n",
    "    cluster_encoded = to_categorical(cluster_labels)\n",
    "    relevance_formated = relevance_labels.astype(\"float32\")\n",
    "\n",
    "    # Remark: Cluster Shape is not word shape... Got confused for a sec. xD\n",
    "    print(\"Cluster-Shape: \",cluster_encoded.shape, \"\\tRelevance-Shape: \", relevance_formated.shape)\n",
    "\n",
    "    return [cluster_encoded, relevance_formated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "976001ea-00a2-43f1-8c76-b9c68202ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "\n",
    "def create_search_model(\n",
    "    text_input_shape,  # Shape of text features\n",
    "    num_clusters      # Number of clusters\n",
    "):\n",
    "    # Single input for text\n",
    "    text_input = Input(shape=text_input_shape, name='text_input')\n",
    "    \n",
    "    # First layer - increased capacity\n",
    "    x = Dense(512, kernel_regularizer=l2(0.0005))(text_input)\n",
    "    x = LeakyReLU(negative_slope=0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Second layer\n",
    "    x = Dense(256, kernel_regularizer=l2(0.0005))(x)\n",
    "    x = LeakyReLU(negative_slope=0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    # Shared layer before branching\n",
    "    x = Dense(128, kernel_regularizer=l2(0.0005))(x)\n",
    "    x = LeakyReLU(negative_slope=0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    shared = Dropout(0.3)(x)\n",
    "    \n",
    "    # Branch for cluster task\n",
    "    cluster_x = Dense(96, kernel_regularizer=l2(0.0005))(shared)\n",
    "    cluster_x = LeakyReLU(negative_slope=0.1)(cluster_x)\n",
    "    cluster_x = BatchNormalization()(cluster_x)\n",
    "    cluster_x = Dropout(0.25)(cluster_x)\n",
    "    cluster_x = Dense(64, kernel_regularizer=l2(0.0005))(cluster_x)\n",
    "    cluster_x = LeakyReLU(negative_slope=0.1)(cluster_x)\n",
    "    cluster_output = Dense(num_clusters, activation='softmax', name='cluster')(cluster_x)\n",
    "    \n",
    "    # Branch for binary relevance task\n",
    "    relevance_x = Dense(96, kernel_regularizer=l2(0.0005))(shared)\n",
    "    relevance_x = LeakyReLU(negative_slope=0.1)(relevance_x)\n",
    "    relevance_x = BatchNormalization()(relevance_x)\n",
    "    relevance_x = Dropout(0.4)(relevance_x)\n",
    "    relevance_x = Dense(48, kernel_regularizer=l2(0.0005))(relevance_x)\n",
    "    relevance_x = LeakyReLU(negative_slope=0.1)(relevance_x)\n",
    "    relevance_x = BatchNormalization()(relevance_x)\n",
    "    relevance_x = Dropout(0.35)(relevance_x)\n",
    "    \n",
    "    # Output layer for binary relevance\n",
    "    relevance_output = Dense(1, activation='sigmoid', name='relevance')(relevance_x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(\n",
    "        inputs=text_input,\n",
    "        outputs=[cluster_output, relevance_output]\n",
    "    )\n",
    "    \n",
    "    # Compile with standard losses\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0008),\n",
    "        loss={\n",
    "            'cluster': 'categorical_crossentropy',\n",
    "            'relevance': 'binary_crossentropy'\n",
    "        },\n",
    "        loss_weights={\n",
    "            'cluster': 0.35,\n",
    "            'relevance': 1.0\n",
    "        },\n",
    "        metrics={\n",
    "            'cluster': ['accuracy'],\n",
    "            'relevance': ['accuracy', Precision(name='precision'), Recall(name='recall'), AUC(name='auc')]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a087fd6c-f79c-405b-83d2-e521b402566f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m \u001b[38;5;66;03m# memory efficiency\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m class_weight\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc # memory efficiency\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
    "\n",
    "def train_model(subset=None, df_train=None, n_words=None):\n",
    "    \"\"\"\n",
    "    Improved training function with better learning rate scheduling and data handling\n",
    "    \"\"\"\n",
    "\n",
    "    # Create models directory\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    # Split data with stratification for both cluster and relevance\n",
    "    # First convert relevance to string to use in stratification\n",
    "    df_train['strat_col'] = df_train['cluster'].astype(str) + '_' + df_train['relevance'].astype(str)\n",
    "    \n",
    "    train_df, temp_df = train_test_split(\n",
    "        df_train, test_size=0.3, stratify=df_train['strat_col'], random_state=42\n",
    "    )\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df, test_size=0.5, stratify=temp_df['strat_col'], random_state=42\n",
    "    )\n",
    "\n",
    "    # Remove Data Frames after splitting data in sets\n",
    "    del df_train, temp_df\n",
    "    gc.collect()\n",
    "    \n",
    "    # Clean up the stratification column\n",
    "    train_df.drop('strat_col', axis=1, inplace=True)\n",
    "    val_df.drop('strat_col', axis=1, inplace=True)\n",
    "    test_df.drop('strat_col', axis=1, inplace=True)\n",
    "    \n",
    "    print(f\"Data split - Train: {train_df.shape}, Validation: {val_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "    # Vectorize data\n",
    "    x_train = vectorize_sequences(train_df[\"term_idx\"], dimension=n_words)\n",
    "    x_val = vectorize_sequences(val_df[\"term_idx\"], dimension=n_words)\n",
    "    x_test = vectorize_sequences(test_df[\"term_idx\"], dimension=n_words)\n",
    "    \n",
    "    # Get labels\n",
    "    y_train = get_label(train_df)\n",
    "    y_val = get_label(val_df)\n",
    "    y_test = get_label(test_df)\n",
    "    \n",
    "    # Calculate class weights for relevance with more emphasis on positive class\n",
    "    relevance_class_weights = class_weight.compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(train_df[\"relevance\"]),\n",
    "        y=train_df[\"relevance\"]\n",
    "    )\n",
    "    # Adjust weights to prioritize precision (penalize false positives more)\n",
    "    relevance_class_weights[1] = relevance_class_weights[1] * 1.5  # Increase weight for positive class\n",
    "    relevance_weight_dict = {i: relevance_class_weights[i] for i in range(len(relevance_class_weights))}\n",
    "    print(f\"Relevance class weights: {relevance_weight_dict}\")\n",
    "\n",
    "    # Remove Data Frames as vectorized data exists\n",
    "    del train_df, val_df, test_df\n",
    "    gc.collect()\n",
    "    \n",
    "    # Create model\n",
    "    text_shape = (n_words,)\n",
    "    num_clusters = len(y_train[0][0])\n",
    "    model = create_search_model(text_shape, num_clusters)\n",
    "    \n",
    "    # Set up callbacks\n",
    "    def cosine_annealing_lr(epoch, initial_lr=0.0008, min_lr=1e-6, total_epochs=100):\n",
    "        \"\"\"Cosine annealing learning rate schedule\"\"\"\n",
    "        import numpy as np\n",
    "        return min_lr + (initial_lr - min_lr) * (1 + np.cos(np.pi * epoch / total_epochs)) / 2\n",
    "    \n",
    "    callbacks = [\n",
    "        # Cosine Annealing learning rate scheduler\n",
    "        LearningRateScheduler(cosine_annealing_lr),\n",
    "        \n",
    "        # Reduce LR on plateau as backup\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_relevance_precision',\n",
    "            factor=0.6,\n",
    "            patience=4,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1,\n",
    "            mode='max'  # We want to maximize precision\n",
    "        ),\n",
    "        \n",
    "        # Early stopping based on validation precision\n",
    "        EarlyStopping(\n",
    "            monitor='val_relevance_precision',\n",
    "            patience=12,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "            mode='max',  # We want to maximize precision\n",
    "            min_delta=0.001\n",
    "        ),\n",
    "        \n",
    "        # Checkpoint based on validation precision\n",
    "        ModelCheckpoint(\n",
    "            f'models/model_callback_best_epoch_{subset}.keras',\n",
    "            monitor='val_relevance_precision',\n",
    "            save_best_only=True,\n",
    "            mode='max',  # We want to maximize precision\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Training parameters\n",
    "    epochs = 100  # Early stopping will determine actual epochs\n",
    "    batch_size = 128\n",
    "    \n",
    "    # Create sample weights for relevance task\n",
    "    sample_weights = np.ones(len(train_df))\n",
    "    for i, rel in enumerate(train_df[\"relevance\"]):\n",
    "        sample_weights[i] = relevance_weight_dict[int(rel)]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        shuffle=True,\n",
    "        sample_weight=sample_weights\n",
    "    )\n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    # Save model with a standard format\n",
    "    model_name = f\"standard_{subset}\"\n",
    "    model.save(f'models/{model_name}.keras')\n",
    "    \n",
    "    # Also save model using SavedModel format which preserves the entire model\n",
    "    tf.keras.models.save_model(model, f'models/model_earlystop_callback_{model_name}.keras')\n",
    "    \n",
    "    # Evaluate on test set with focus on precision metrics\n",
    "    results = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    metric_names = model.metrics_names\n",
    "    \n",
    "    for i, metric_name in enumerate(metric_names):\n",
    "        print(f\"{metric_name}: {results[i]:.4f}\")\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = model.predict(x_test)\n",
    "    \n",
    "    # Compute F1 score for cluster predictions\n",
    "    from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "    \n",
    "    # Get the most likely cluster\n",
    "    y_true_cluster = np.argmax(y_test[0], axis=1)\n",
    "    y_pred_cluster = np.argmax(predictions[0], axis=1)\n",
    "    \n",
    "    # Compute macro and weighted F1\n",
    "    cluster_f1_macro = f1_score(y_true_cluster, y_pred_cluster, average='macro')\n",
    "    cluster_f1_weighted = f1_score(y_true_cluster, y_pred_cluster, average='weighted')\n",
    "    \n",
    "    print(f\"Cluster F1 (macro): {cluster_f1_macro:.4f}\")\n",
    "    print(f\"Cluster F1 (weighted): {cluster_f1_weighted:.4f}\")\n",
    "    \n",
    "    # Compute precision and recall for relevance\n",
    "    y_true_relevance = y_test[1]\n",
    "    y_pred_relevance = (predictions[1] > 0.5).astype(int).flatten()\n",
    "    \n",
    "    relevance_precision = precision_score(y_true_relevance, y_pred_relevance)\n",
    "    relevance_recall = recall_score(y_true_relevance, y_pred_relevance)\n",
    "    relevance_f1 = f1_score(y_true_relevance, y_pred_relevance)\n",
    "    \n",
    "    print(f\"Relevance Precision: {relevance_precision:.4f}\")\n",
    "    print(f\"Relevance Recall: {relevance_recall:.4f}\")\n",
    "    print(f\"Relevance F1: {relevance_f1:.4f}\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65f88ef8-8dfd-4d7b-8b8a-6b586c3ef377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(subset: str = None, n_words=None):\n",
    "    df_train = get_subsetdata(subset=subset)\n",
    "    print(df_train)\n",
    "    df_train[\"term_idx\"] = df_train[\"term_idx\"].apply(lambda x: list(x))\n",
    "    if not df_train.empty:\n",
    "        train_model(subset=subset, df_train=df_train, n_words=n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa3051ae-ddc1-4885-8a1d-5097a492bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sub_collection and count(*) for each\n",
    "#query= \"\"\"\n",
    "#select a.sub_collection, count(*)\n",
    "#from \"Document\" a\n",
    "#group by a.sub_collection\n",
    "#\"\"\"\n",
    "#df_subcol_count = sql_query(query)\n",
    "#print(df_subcol_count)\n",
    "#subcollections = df_subcol_count[\"sub_collection\"].tolist()\n",
    "\n",
    "subcollections = [\"all_subcollections\"] #['2022-06', '2022-07', '2022-09', '2023-01', '2023-06', '2023-08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0ab0814-5574-4212-9a09-7512ab21c238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model for:  all_subcollections\n",
      "Found saved type information for 5 columns\n",
      "Warning: Column 'id' has saved type 'string' but detected as 'object'\n",
      "Warning: Column 'docid' has saved type 'string' but detected as 'object'\n",
      "Warning: Column 'cluster' has saved type 'numeric' but detected as 'int'\n",
      "          docid                                           term_idx  relevance  \\\n",
      "0      doc10189  [3290, 403, 899, 2863, 49, 137, 2735, 1476, 3,...          0   \n",
      "1      doc10189  [3290, 403, 899, 2863, 49, 137, 2735, 1476, 3,...          1   \n",
      "2      doc10189  [3290, 403, 899, 2863, 49, 137, 2735, 1476, 3,...          0   \n",
      "3      doc10189  [3290, 403, 899, 2863, 49, 137, 2735, 1476, 3,...          0   \n",
      "4      doc10019  [5579, 2, 2130, 678, 1155, 4528, 108, 209, 371...          0   \n",
      "...         ...                                                ...        ...   \n",
      "64883   doc9963  [6850, 7516, 0, 1829, 350, 4065, 393, 14, 453,...          1   \n",
      "64884   doc9974  [4512, 360, 3, 85, 6243, 360, 1653, 3, 85, 451...          1   \n",
      "64885   doc9974  [4512, 360, 3, 85, 6243, 360, 1653, 3, 85, 451...          0   \n",
      "64886   doc9974  [4512, 360, 3, 85, 6243, 360, 1653, 3, 85, 451...          1   \n",
      "64887   doc9974  [4512, 360, 3, 85, 6243, 360, 1653, 3, 85, 451...          1   \n",
      "\n",
      "       cluster  \n",
      "0           35  \n",
      "1           35  \n",
      "2           48  \n",
      "3           20  \n",
      "4           15  \n",
      "...        ...  \n",
      "64883       50  \n",
      "64884       20  \n",
      "64885       50  \n",
      "64886        8  \n",
      "64887        1  \n",
      "\n",
      "[64888 rows x 4 columns]\n",
      "Data split - Train: (45421, 4), Validation: (9733, 4), Test: (9734, 4)\n",
      "Cluster-Shape:  (45421, 56) \tRelevance-Shape:  (45421,)\n",
      "Cluster-Shape:  (9733, 56) \tRelevance-Shape:  (9733,)\n",
      "Cluster-Shape:  (9734, 56) \tRelevance-Shape:  (9734,)\n",
      "Relevance class weights: {0: np.float64(0.8648653794889372), 1: np.float64(1.7777763281494625)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 15:21:00.516582: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - cluster_accuracy: 0.0623 - cluster_loss: 4.9392 - loss: 3.6361 - relevance_accuracy: 0.4926 - relevance_auc: 0.5204 - relevance_loss: 0.9936 - relevance_precision: 0.4361 - relevance_recall: 0.6558\n",
      "Epoch 1: val_relevance_precision improved from -inf to 0.42660, saving model to models/model_callback_best_epoch_all_subcollections.keras\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - cluster_accuracy: 0.0623 - cluster_loss: 4.9384 - loss: 3.6357 - relevance_accuracy: 0.4926 - relevance_auc: 0.5204 - relevance_loss: 0.9934 - relevance_precision: 0.4361 - relevance_recall: 0.6559 - val_cluster_accuracy: 0.1488 - val_cluster_loss: 3.4295 - val_loss: 2.7885 - val_relevance_accuracy: 0.4400 - val_relevance_auc: 0.5703 - val_relevance_loss: 0.7321 - val_relevance_precision: 0.4266 - val_relevance_recall: 0.9494 - learning_rate: 8.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - cluster_accuracy: 0.1523 - cluster_loss: 4.2258 - loss: 3.1494 - relevance_accuracy: 0.4890 - relevance_auc: 0.5612 - relevance_loss: 0.8543 - relevance_precision: 0.4417 - relevance_recall: 0.7995\n",
      "Epoch 2: val_relevance_precision improved from 0.42660 to 0.45530, saving model to models/model_callback_best_epoch_all_subcollections.keras\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 39ms/step - cluster_accuracy: 0.1524 - cluster_loss: 4.2256 - loss: 3.1492 - relevance_accuracy: 0.4890 - relevance_auc: 0.5612 - relevance_loss: 0.8543 - relevance_precision: 0.4417 - relevance_recall: 0.7995 - val_cluster_accuracy: 0.1962 - val_cluster_loss: 3.1881 - val_loss: 2.5292 - val_relevance_accuracy: 0.5117 - val_relevance_auc: 0.6022 - val_relevance_loss: 0.7081 - val_relevance_precision: 0.4553 - val_relevance_recall: 0.7997 - learning_rate: 7.9980e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m354/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - cluster_accuracy: 0.2091 - cluster_loss: 3.8714 - loss: 2.8394 - relevance_accuracy: 0.5441 - relevance_auc: 0.6488 - relevance_loss: 0.8034 - relevance_precision: 0.4784 - relevance_recall: 0.8195\n",
      "Epoch 3: val_relevance_precision improved from 0.45530 to 0.46249, saving model to models/model_callback_best_epoch_all_subcollections.keras\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - cluster_accuracy: 0.2091 - cluster_loss: 3.8714 - loss: 2.8393 - relevance_accuracy: 0.5441 - relevance_auc: 0.6487 - relevance_loss: 0.8035 - relevance_precision: 0.4783 - relevance_recall: 0.8195 - val_cluster_accuracy: 0.2243 - val_cluster_loss: 3.0616 - val_loss: 2.4040 - val_relevance_accuracy: 0.5255 - val_relevance_auc: 0.5999 - val_relevance_loss: 0.7096 - val_relevance_precision: 0.4625 - val_relevance_recall: 0.7653 - learning_rate: 7.9901e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m354/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - cluster_accuracy: 0.2468 - cluster_loss: 3.6141 - loss: 2.6481 - relevance_accuracy: 0.6056 - relevance_auc: 0.7044 - relevance_loss: 0.7655 - relevance_precision: 0.5224 - relevance_recall: 0.8388\n",
      "Epoch 4: val_relevance_precision improved from 0.46249 to 0.47873, saving model to models/model_callback_best_epoch_all_subcollections.keras\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 40ms/step - cluster_accuracy: 0.2468 - cluster_loss: 3.6143 - loss: 2.6483 - relevance_accuracy: 0.6055 - relevance_auc: 0.7043 - relevance_loss: 0.7655 - relevance_precision: 0.5223 - relevance_recall: 0.8387 - val_cluster_accuracy: 0.2324 - val_cluster_loss: 3.0125 - val_loss: 2.3657 - val_relevance_accuracy: 0.5529 - val_relevance_auc: 0.5983 - val_relevance_loss: 0.7034 - val_relevance_precision: 0.4787 - val_relevance_recall: 0.6685 - learning_rate: 7.9724e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m354/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - cluster_accuracy: 0.2783 - cluster_loss: 3.4218 - loss: 2.5355 - relevance_accuracy: 0.6431 - relevance_auc: 0.7447 - relevance_loss: 0.7271 - relevance_precision: 0.5523 - relevance_recall: 0.8466\n",
      "Epoch 5: val_relevance_precision did not improve from 0.47873\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - cluster_accuracy: 0.2783 - cluster_loss: 3.4221 - loss: 2.5358 - relevance_accuracy: 0.6430 - relevance_auc: 0.7446 - relevance_loss: 0.7272 - relevance_precision: 0.5522 - relevance_recall: 0.8465 - val_cluster_accuracy: 0.2374 - val_cluster_loss: 2.9973 - val_loss: 2.3953 - val_relevance_accuracy: 0.5481 - val_relevance_auc: 0.6055 - val_relevance_loss: 0.7206 - val_relevance_precision: 0.4756 - val_relevance_recall: 0.6894 - learning_rate: 7.9410e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - cluster_accuracy: 0.3038 - cluster_loss: 3.2514 - loss: 2.4559 - relevance_accuracy: 0.6758 - relevance_auc: 0.7775 - relevance_loss: 0.6867 - relevance_precision: 0.5760 - relevance_recall: 0.8636\n",
      "Epoch 6: val_relevance_precision improved from 0.47873 to 0.47954, saving model to models/model_callback_best_epoch_all_subcollections.keras\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - cluster_accuracy: 0.3037 - cluster_loss: 3.2517 - loss: 2.4562 - relevance_accuracy: 0.6757 - relevance_auc: 0.7774 - relevance_loss: 0.6868 - relevance_precision: 0.5760 - relevance_recall: 0.8635 - val_cluster_accuracy: 0.2438 - val_cluster_loss: 2.9984 - val_loss: 2.4230 - val_relevance_accuracy: 0.5556 - val_relevance_auc: 0.5946 - val_relevance_loss: 0.7133 - val_relevance_precision: 0.4795 - val_relevance_recall: 0.6190 - learning_rate: 7.8922e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - cluster_accuracy: 0.3175 - cluster_loss: 3.1683 - loss: 2.4304 - relevance_accuracy: 0.6849 - relevance_auc: 0.7938 - relevance_loss: 0.6635 - relevance_precision: 0.5823 - relevance_recall: 0.8848\n",
      "Epoch 7: val_relevance_precision did not improve from 0.47954\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - cluster_accuracy: 0.3174 - cluster_loss: 3.1686 - loss: 2.4306 - relevance_accuracy: 0.6848 - relevance_auc: 0.7938 - relevance_loss: 0.6635 - relevance_precision: 0.5822 - relevance_recall: 0.8847 - val_cluster_accuracy: 0.2451 - val_cluster_loss: 2.9969 - val_loss: 2.4854 - val_relevance_accuracy: 0.5397 - val_relevance_auc: 0.5984 - val_relevance_loss: 0.7461 - val_relevance_precision: 0.4699 - val_relevance_recall: 0.7057 - learning_rate: 7.8224e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m354/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - cluster_accuracy: 0.3363 - cluster_loss: 3.0732 - loss: 2.4011 - relevance_accuracy: 0.7020 - relevance_auc: 0.8104 - relevance_loss: 0.6387 - relevance_precision: 0.5990 - relevance_recall: 0.9008\n",
      "Epoch 8: val_relevance_precision improved from 0.47954 to 0.48338, saving model to models/model_callback_best_epoch_all_subcollections.keras\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - cluster_accuracy: 0.3362 - cluster_loss: 3.0738 - loss: 2.4015 - relevance_accuracy: 0.7019 - relevance_auc: 0.8103 - relevance_loss: 0.6389 - relevance_precision: 0.5989 - relevance_recall: 0.9007 - val_cluster_accuracy: 0.2469 - val_cluster_loss: 3.0254 - val_loss: 2.5110 - val_relevance_accuracy: 0.5601 - val_relevance_auc: 0.5960 - val_relevance_loss: 0.7407 - val_relevance_precision: 0.4834 - val_relevance_recall: 0.6159 - learning_rate: 7.7283e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - cluster_accuracy: 0.3520 - cluster_loss: 2.9923 - loss: 2.3750 - relevance_accuracy: 0.7144 - relevance_auc: 0.8207 - relevance_loss: 0.6203 - relevance_precision: 0.6096 - relevance_recall: 0.9048\n",
      "Epoch 9: val_relevance_precision did not improve from 0.48338\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 42ms/step - cluster_accuracy: 0.3519 - cluster_loss: 2.9926 - loss: 2.3753 - relevance_accuracy: 0.7143 - relevance_auc: 0.8206 - relevance_loss: 0.6204 - relevance_precision: 0.6095 - relevance_recall: 0.9048 - val_cluster_accuracy: 0.2483 - val_cluster_loss: 3.0098 - val_loss: 2.5619 - val_relevance_accuracy: 0.5447 - val_relevance_auc: 0.5931 - val_relevance_loss: 0.7714 - val_relevance_precision: 0.4726 - val_relevance_recall: 0.6787 - learning_rate: 7.6071e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - cluster_accuracy: 0.3674 - cluster_loss: 2.9070 - loss: 2.3450 - relevance_accuracy: 0.7216 - relevance_auc: 0.8320 - relevance_loss: 0.6020 - relevance_precision: 0.6151 - relevance_recall: 0.9087\n",
      "Epoch 10: val_relevance_precision did not improve from 0.48338\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 36ms/step - cluster_accuracy: 0.3674 - cluster_loss: 2.9074 - loss: 2.3452 - relevance_accuracy: 0.7216 - relevance_auc: 0.8320 - relevance_loss: 0.6021 - relevance_precision: 0.6150 - relevance_recall: 0.9087 - val_cluster_accuracy: 0.2462 - val_cluster_loss: 3.0161 - val_loss: 2.5652 - val_relevance_accuracy: 0.5597 - val_relevance_auc: 0.5979 - val_relevance_loss: 0.7648 - val_relevance_precision: 0.4830 - val_relevance_recall: 0.6139 - learning_rate: 7.4563e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m354/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - cluster_accuracy: 0.3775 - cluster_loss: 2.8358 - loss: 2.3141 - relevance_accuracy: 0.7265 - relevance_auc: 0.8403 - relevance_loss: 0.5885 - relevance_precision: 0.6169 - relevance_recall: 0.9101\n",
      "Epoch 11: val_relevance_precision improved from 0.48338 to 0.48443, saving model to models/model_callback_best_epoch_all_subcollections.keras\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - cluster_accuracy: 0.3774 - cluster_loss: 2.8365 - loss: 2.3145 - relevance_accuracy: 0.7264 - relevance_auc: 0.8402 - relevance_loss: 0.5887 - relevance_precision: 0.6169 - relevance_recall: 0.9101 - val_cluster_accuracy: 0.2540 - val_cluster_loss: 3.0393 - val_loss: 2.5991 - val_relevance_accuracy: 0.5618 - val_relevance_auc: 0.5963 - val_relevance_loss: 0.7780 - val_relevance_precision: 0.4844 - val_relevance_recall: 0.5944 - learning_rate: 7.2740e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m354/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - cluster_accuracy: 0.3932 - cluster_loss: 2.7491 - loss: 2.2800 - relevance_accuracy: 0.7356 - relevance_auc: 0.8486 - relevance_loss: 0.5738 - relevance_precision: 0.6265 - relevance_recall: 0.9146\n",
      "Epoch 12: val_relevance_precision did not improve from 0.48443\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - cluster_accuracy: 0.3931 - cluster_loss: 2.7499 - loss: 2.2805 - relevance_accuracy: 0.7355 - relevance_auc: 0.8485 - relevance_loss: 0.5739 - relevance_precision: 0.6264 - relevance_recall: 0.9145 - val_cluster_accuracy: 0.2525 - val_cluster_loss: 3.0398 - val_loss: 2.6246 - val_relevance_accuracy: 0.5478 - val_relevance_auc: 0.5897 - val_relevance_loss: 0.7944 - val_relevance_precision: 0.4734 - val_relevance_recall: 0.6351 - learning_rate: 7.0593e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - cluster_accuracy: 0.4023 - cluster_loss: 2.7022 - loss: 2.2552 - relevance_accuracy: 0.7419 - relevance_auc: 0.8546 - relevance_loss: 0.5628 - relevance_precision: 0.6342 - relevance_recall: 0.9174\n",
      "Epoch 13: val_relevance_precision did not improve from 0.48443\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 41ms/step - cluster_accuracy: 0.4022 - cluster_loss: 2.7026 - loss: 2.2554 - relevance_accuracy: 0.7419 - relevance_auc: 0.8545 - relevance_loss: 0.5629 - relevance_precision: 0.6342 - relevance_recall: 0.9174 - val_cluster_accuracy: 0.2549 - val_cluster_loss: 3.0436 - val_loss: 2.6381 - val_relevance_accuracy: 0.5610 - val_relevance_auc: 0.5928 - val_relevance_loss: 0.8114 - val_relevance_precision: 0.4841 - val_relevance_recall: 0.6130 - learning_rate: 6.8118e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m354/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - cluster_accuracy: 0.4160 - cluster_loss: 2.6457 - loss: 2.2186 - relevance_accuracy: 0.7472 - relevance_auc: 0.8615 - relevance_loss: 0.5482 - relevance_precision: 0.6414 - relevance_recall: 0.9209\n",
      "Epoch 14: val_relevance_precision did not improve from 0.48443\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 39ms/step - cluster_accuracy: 0.4159 - cluster_loss: 2.6462 - loss: 2.2190 - relevance_accuracy: 0.7472 - relevance_auc: 0.8614 - relevance_loss: 0.5484 - relevance_precision: 0.6413 - relevance_recall: 0.9208 - val_cluster_accuracy: 0.2521 - val_cluster_loss: 3.0845 - val_loss: 2.6625 - val_relevance_accuracy: 0.5502 - val_relevance_auc: 0.5918 - val_relevance_loss: 0.8244 - val_relevance_precision: 0.4757 - val_relevance_recall: 0.6439 - learning_rate: 6.5321e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m354/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - cluster_accuracy: 0.4299 - cluster_loss: 2.5664 - loss: 2.1737 - relevance_accuracy: 0.7554 - relevance_auc: 0.8702 - relevance_loss: 0.5339 - relevance_precision: 0.6466 - relevance_recall: 0.9244\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0003733044024556875.\n",
      "\n",
      "Epoch 15: val_relevance_precision did not improve from 0.48443\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - cluster_accuracy: 0.4297 - cluster_loss: 2.5671 - loss: 2.1741 - relevance_accuracy: 0.7553 - relevance_auc: 0.8701 - relevance_loss: 0.5341 - relevance_precision: 0.6465 - relevance_recall: 0.9244 - val_cluster_accuracy: 0.2560 - val_cluster_loss: 3.0805 - val_loss: 2.6663 - val_relevance_accuracy: 0.5522 - val_relevance_auc: 0.5895 - val_relevance_loss: 0.8385 - val_relevance_precision: 0.4776 - val_relevance_recall: 0.6495 - learning_rate: 6.2217e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - cluster_accuracy: 0.4632 - cluster_loss: 2.3920 - loss: 2.0577 - relevance_accuracy: 0.7722 - relevance_auc: 0.8862 - relevance_loss: 0.5005 - relevance_precision: 0.6632 - relevance_recall: 0.9343\n",
      "Epoch 16: val_relevance_precision improved from 0.48443 to 0.49318, saving model to models/model_callback_best_epoch_all_subcollections.keras\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - cluster_accuracy: 0.4632 - cluster_loss: 2.3921 - loss: 2.0577 - relevance_accuracy: 0.7722 - relevance_auc: 0.8862 - relevance_loss: 0.5006 - relevance_precision: 0.6632 - relevance_recall: 0.9343 - val_cluster_accuracy: 0.2601 - val_cluster_loss: 3.1460 - val_loss: 2.7423 - val_relevance_accuracy: 0.5711 - val_relevance_auc: 0.5930 - val_relevance_loss: 0.9670 - val_relevance_precision: 0.4932 - val_relevance_recall: 0.5811 - learning_rate: 3.5302e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m354/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - cluster_accuracy: 0.5015 - cluster_loss: 2.2029 - loss: 1.8884 - relevance_accuracy: 0.7858 - relevance_auc: 0.8982 - relevance_loss: 0.4646 - relevance_precision: 0.6759 - relevance_recall: 0.9497\n",
      "Epoch 17: val_relevance_precision did not improve from 0.49318\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - cluster_accuracy: 0.5014 - cluster_loss: 2.2033 - loss: 1.8886 - relevance_accuracy: 0.7858 - relevance_auc: 0.8982 - relevance_loss: 0.4646 - relevance_precision: 0.6759 - relevance_recall: 0.9496 - val_cluster_accuracy: 0.2581 - val_cluster_loss: 3.2117 - val_loss: 2.8016 - val_relevance_accuracy: 0.5654 - val_relevance_auc: 0.5892 - val_relevance_loss: 1.0454 - val_relevance_precision: 0.4876 - val_relevance_recall: 0.5825 - learning_rate: 3.3124e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m354/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - cluster_accuracy: 0.5200 - cluster_loss: 2.1067 - loss: 1.7998 - relevance_accuracy: 0.7897 - relevance_auc: 0.9045 - relevance_loss: 0.4487 - relevance_precision: 0.6798 - relevance_recall: 0.9507\n",
      "Epoch 18: val_relevance_precision did not improve from 0.49318\n",
      "\u001b[1m355/355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 39ms/step - cluster_accuracy: 0.5199 - cluster_loss: 2.1072 - loss: 1.8000 - relevance_accuracy: 0.7897 - relevance_auc: 0.9045 - relevance_loss: 0.4488 - relevance_precision: 0.6798 - relevance_recall: 0.9507 - val_cluster_accuracy: 0.2558 - val_cluster_loss: 3.2975 - val_loss: 2.8458 - val_relevance_accuracy: 0.5686 - val_relevance_auc: 0.5866 - val_relevance_loss: 1.0862 - val_relevance_precision: 0.4908 - val_relevance_recall: 0.5879 - learning_rate: 3.0825e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m223/355\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - cluster_accuracy: 0.5551 - cluster_loss: 1.9503 - loss: 1.7023 - relevance_accuracy: 0.7963 - relevance_auc: 0.9134 - relevance_loss: 0.4300 - relevance_precision: 0.6831 - relevance_recall: 0.9483"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, subcollection \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(subcollections, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating Model for: \u001b[39m\u001b[38;5;124m\"\u001b[39m, subcollection)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubcollection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10_000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m!!!DONE!!!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(subset, n_words)\u001b[0m\n\u001b[1;32m      4\u001b[0m df_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterm_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterm_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlist\u001b[39m(x))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df_train\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_words\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 109\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(subset, df_train, n_words)\u001b[0m\n\u001b[1;32m    106\u001b[0m     sample_weights[i] \u001b[38;5;241m=\u001b[39m relevance_weight_dict[\u001b[38;5;28mint\u001b[39m(rel)]\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Save model with a standard format\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m     ):\n\u001b[0;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1703\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for idx, subcollection in enumerate(subcollections, start=1):\n",
    "    print(\"Creating Model for: \", subcollection)\n",
    "    main(subset=subcollection, n_words=10_000)\n",
    "\n",
    "print(\"\\n!!!DONE!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62179ae5-3e98-4020-9c72-2d814bd99749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
