{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25216d7-40c6-41f9-a8d3-ed4c4d4d0c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get combined unqique queries from test collection as txt\n",
    "import os\n",
    "\n",
    "file_paths = [\n",
    "    \"datasets/LongEval-Web/LongEval Test Collection/queries/2023-03_queries.txt\",\n",
    "    \"datasets/LongEval-Web/LongEval Test Collection/queries/2023-04_queries.txt\",\n",
    "    \"datasets/LongEval-Web/LongEval Test Collection/queries/2023-05_queries.txt\",\n",
    "    \"datasets/LongEval-Web/LongEval Test Collection/queries/2023-06_queries.txt\",\n",
    "    \"datasets/LongEval-Web/LongEval Test Collection/queries/2023-07_queries.txt\",\n",
    "    \"datasets/LongEval-Web/LongEval Test Collection/queries/2023-08_queries.txt\"\n",
    "]\n",
    "\n",
    "# Use a dictionary to ensure unique qid (keeps the first query seen for each qid)\n",
    "qid_to_query = {}\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            qid, query = line.split(\"\\t\", 1)\n",
    "            qid = qid.strip()\n",
    "            query = query.strip()\n",
    "            if qid not in qid_to_query:\n",
    "                qid_to_query[qid] = query  # Keep the first seen\n",
    "\n",
    "# Optional: sort by numeric qid\n",
    "sorted_entries = sorted(qid_to_query.items(), key=lambda x: int(x[0]))\n",
    "\n",
    "# Write to output file\n",
    "with open(\"combined_unique_queries.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for qid, query in sorted_entries:\n",
    "        f.write(f\"{qid}\\t{query}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7081798c-0f9b-44a2-8bfb-708f7e98f5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"combined_unique_queries.txt\", sep=\"\\t\", names=[\"qid\", \"query\"]).to_csv(\"unique_queries_2023-03_to_2023-08.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f0ed937-5d63-440c-980a-e2f6173b697d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 rows processed...\n",
      "200 rows processed...\n",
      "300 rows processed...\n",
      "400 rows processed...\n",
      "500 rows processed...\n",
      "600 rows processed...\n",
      "700 rows processed...\n",
      "800 rows processed...\n",
      "900 rows processed...\n",
      "1000 rows processed...\n",
      "1100 rows processed...\n",
      "1200 rows processed...\n",
      "1300 rows processed...\n",
      "1400 rows processed...\n",
      "1500 rows processed...\n",
      "1600 rows processed...\n",
      "1700 rows processed...\n",
      "1800 rows processed...\n",
      "1900 rows processed...\n",
      "2000 rows processed...\n",
      "2100 rows processed...\n",
      "2200 rows processed...\n",
      "2300 rows processed...\n",
      "2400 rows processed...\n",
      "2500 rows processed...\n",
      "2600 rows processed...\n",
      "2700 rows processed...\n",
      "2800 rows processed...\n",
      "2900 rows processed...\n",
      "3000 rows processed...\n",
      "3100 rows processed...\n",
      "3200 rows processed...\n",
      "3300 rows processed...\n",
      "3400 rows processed...\n",
      "3500 rows processed...\n",
      "3600 rows processed...\n",
      "3700 rows processed...\n",
      "3800 rows processed...\n",
      "3900 rows processed...\n",
      "4000 rows processed...\n",
      "4100 rows processed...\n",
      "4200 rows processed...\n",
      "4300 rows processed...\n",
      "4400 rows processed...\n",
      "4500 rows processed...\n",
      "4600 rows processed...\n",
      "4700 rows processed...\n",
      "4800 rows processed...\n",
      "4900 rows processed...\n",
      "5000 rows processed...\n",
      "5100 rows processed...\n",
      "5200 rows processed...\n",
      "5300 rows processed...\n",
      "5400 rows processed...\n",
      "5500 rows processed...\n",
      "5600 rows processed...\n",
      "5700 rows processed...\n",
      "5800 rows processed...\n",
      "5900 rows processed...\n",
      "6000 rows processed...\n",
      "6100 rows processed...\n",
      "6200 rows processed...\n",
      "6300 rows processed...\n",
      "6400 rows processed...\n",
      "6500 rows processed...\n",
      "6600 rows processed...\n",
      "6700 rows processed...\n",
      "6800 rows processed...\n",
      "6900 rows processed...\n",
      "7000 rows processed...\n",
      "7100 rows processed...\n",
      "7200 rows processed...\n",
      "7300 rows processed...\n",
      "7400 rows processed...\n",
      "7500 rows processed...\n",
      "7600 rows processed...\n",
      "7700 rows processed...\n",
      "7800 rows processed...\n",
      "7900 rows processed...\n",
      "8000 rows processed...\n",
      "8100 rows processed...\n",
      "8200 rows processed...\n",
      "8300 rows processed...\n",
      "8400 rows processed...\n",
      "8500 rows processed...\n",
      "8600 rows processed...\n",
      "8700 rows processed...\n",
      "8800 rows processed...\n",
      "8900 rows processed...\n",
      "9000 rows processed...\n",
      "9100 rows processed...\n",
      "9200 rows processed...\n",
      "9300 rows processed...\n",
      "9400 rows processed...\n",
      "9500 rows processed...\n",
      "9600 rows processed...\n",
      "9700 rows processed...\n",
      "9800 rows processed...\n",
      "9900 rows processed...\n",
      "10000 rows processed...\n",
      "10100 rows processed...\n",
      "10200 rows processed...\n",
      "10300 rows processed...\n",
      "10400 rows processed...\n",
      "10500 rows processed...\n",
      "10600 rows processed...\n",
      "10700 rows processed...\n",
      "10800 rows processed...\n",
      "10900 rows processed...\n",
      "11000 rows processed...\n",
      "11100 rows processed...\n",
      "11200 rows processed...\n",
      "11300 rows processed...\n",
      "11400 rows processed...\n",
      "11500 rows processed...\n",
      "11600 rows processed...\n",
      "11700 rows processed...\n",
      "11800 rows processed...\n",
      "11900 rows processed...\n",
      "12000 rows processed...\n",
      "12100 rows processed...\n",
      "12200 rows processed...\n",
      "12300 rows processed...\n",
      "12400 rows processed...\n",
      "12500 rows processed...\n",
      "12600 rows processed...\n",
      "12700 rows processed...\n",
      "12800 rows processed...\n",
      "12900 rows processed...\n",
      "13000 rows processed...\n",
      "13100 rows processed...\n",
      "13200 rows processed...\n",
      "13300 rows processed...\n",
      "13400 rows processed...\n",
      "13500 rows processed...\n",
      "13600 rows processed...\n",
      "13700 rows processed...\n",
      "13800 rows processed...\n",
      "13900 rows processed...\n",
      "14000 rows processed...\n",
      "14100 rows processed...\n",
      "14200 rows processed...\n",
      "14300 rows processed...\n",
      "14400 rows processed...\n",
      "14500 rows processed...\n",
      "14600 rows processed...\n",
      "14700 rows processed...\n",
      "14800 rows processed...\n",
      "14900 rows processed...\n",
      "15000 rows processed...\n",
      "15100 rows processed...\n",
      "15200 rows processed...\n",
      "15300 rows processed...\n",
      "15400 rows processed...\n",
      "15500 rows processed...\n",
      "15600 rows processed...\n",
      "15700 rows processed...\n",
      "15800 rows processed...\n",
      "15900 rows processed...\n",
      "16000 rows processed...\n",
      "16100 rows processed...\n",
      "16200 rows processed...\n",
      "16300 rows processed...\n",
      "16400 rows processed...\n",
      "16500 rows processed...\n",
      "16600 rows processed...\n",
      "16700 rows processed...\n",
      "16800 rows processed...\n",
      "16900 rows processed...\n",
      "17000 rows processed...\n",
      "17100 rows processed...\n",
      "17200 rows processed...\n",
      "17300 rows processed...\n",
      "17400 rows processed...\n",
      "17500 rows processed...\n",
      "17600 rows processed...\n",
      "17700 rows processed...\n",
      "17800 rows processed...\n",
      "17900 rows processed...\n",
      "18000 rows processed...\n",
      "18100 rows processed...\n",
      "18200 rows processed...\n",
      "18300 rows processed...\n",
      "18400 rows processed...\n",
      "18500 rows processed...\n",
      "18600 rows processed...\n",
      "18700 rows processed...\n",
      "18800 rows processed...\n",
      "18900 rows processed...\n",
      "19000 rows processed...\n",
      "19100 rows processed...\n",
      "19200 rows processed...\n",
      "19300 rows processed...\n",
      "19400 rows processed...\n",
      "19500 rows processed...\n",
      "19600 rows processed...\n",
      "19700 rows processed...\n",
      "19800 rows processed...\n",
      "19900 rows processed...\n",
      "20000 rows processed...\n",
      "20100 rows processed...\n",
      "20200 rows processed...\n",
      "20300 rows processed...\n",
      "20400 rows processed...\n",
      "20500 rows processed...\n",
      "20600 rows processed...\n",
      "20700 rows processed...\n",
      "20800 rows processed...\n",
      "20900 rows processed...\n",
      "21000 rows processed...\n",
      "21100 rows processed...\n",
      "21200 rows processed...\n",
      "21300 rows processed...\n",
      "21400 rows processed...\n",
      "21500 rows processed...\n",
      "21600 rows processed...\n",
      "21700 rows processed...\n",
      "21800 rows processed...\n",
      "21900 rows processed...\n",
      "22000 rows processed...\n",
      "22100 rows processed...\n",
      "22200 rows processed...\n",
      "22300 rows processed...\n",
      "22400 rows processed...\n",
      "22500 rows processed...\n",
      "22600 rows processed...\n",
      "22700 rows processed...\n",
      "22800 rows processed...\n",
      "22900 rows processed...\n",
      "23000 rows processed...\n",
      "23100 rows processed...\n",
      "23200 rows processed...\n",
      "23300 rows processed...\n",
      "23400 rows processed...\n",
      "23500 rows processed...\n",
      "23600 rows processed...\n",
      "23700 rows processed...\n",
      "23800 rows processed...\n",
      "23900 rows processed...\n",
      "24000 rows processed...\n",
      "24100 rows processed...\n",
      "24200 rows processed...\n",
      "24300 rows processed...\n",
      "24400 rows processed...\n",
      "24500 rows processed...\n",
      "24600 rows processed...\n",
      "24700 rows processed...\n",
      "24800 rows processed...\n",
      "24900 rows processed...\n",
      "25000 rows processed...\n",
      "25100 rows processed...\n",
      "25200 rows processed...\n",
      "25300 rows processed...\n",
      "25400 rows processed...\n",
      "25500 rows processed...\n",
      "25600 rows processed...\n",
      "25700 rows processed...\n",
      "25800 rows processed...\n",
      "25900 rows processed...\n",
      "26000 rows processed...\n",
      "26100 rows processed...\n",
      "26200 rows processed...\n",
      "26300 rows processed...\n",
      "26400 rows processed...\n",
      "26500 rows processed...\n",
      "26600 rows processed...\n",
      "26700 rows processed...\n",
      "26800 rows processed...\n",
      "26900 rows processed...\n",
      "27000 rows processed...\n",
      "27100 rows processed...\n",
      "27200 rows processed...\n",
      "27300 rows processed...\n",
      "27400 rows processed...\n",
      "27500 rows processed...\n",
      "27600 rows processed...\n",
      "27700 rows processed...\n",
      "27800 rows processed...\n",
      "27900 rows processed...\n",
      "28000 rows processed...\n",
      "28100 rows processed...\n",
      "28200 rows processed...\n",
      "28300 rows processed...\n",
      "28400 rows processed...\n",
      "28500 rows processed...\n",
      "28600 rows processed...\n",
      "28700 rows processed...\n",
      "28800 rows processed...\n",
      "28900 rows processed...\n",
      "29000 rows processed...\n",
      "29100 rows processed...\n",
      "29200 rows processed...\n",
      "29300 rows processed...\n",
      "29400 rows processed...\n",
      "29500 rows processed...\n",
      "29600 rows processed...\n",
      "29700 rows processed...\n",
      "29800 rows processed...\n",
      "29900 rows processed...\n",
      "30000 rows processed...\n",
      "30100 rows processed...\n",
      "30200 rows processed...\n",
      "Updated CSV file created at: unique_queries_2023-03_to_2023-08_updated.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "import os\n",
    "\n",
    "api_key = \"\"\n",
    "csv_file_path = 'unique_queries_2023-03_to_2023-08.csv'\n",
    "api_url = \"https://api.openai.com/v1/chat/completions\"\n",
    "model_name = \"gpt-4o-mini\"  \n",
    "max_length = 200\n",
    "rows = []\n",
    "row_counter = 1\n",
    "base, ext = os.path.splitext(csv_file_path)\n",
    "new_csv_file_path = f\"{base}_updated{ext}\"\n",
    "context = \"Try to be sure about it. Categorize the request into one of the following categories, responding only with the category name: time-independent (timeless information not tied to a specific time or event, e.g., definitions, recipes, general rules), explicit-time (requests with explicit time references, e.g., years, dates, specific periods), event (requests about specific events, e.g., Named public events, Scheduled institutional events, Historical events), or timeliness (time-sensitive or current information where up-to-date info or availability matters e.g., weather, stock prices, live updates, buying intent, tax rates). Categorization Process: 1. Look for explicit time references (e.g., years, dates). Assign to explicit-time if present. 2. Check for event-related terms. Assign to event if applicable. 3. If the request requires real-time or current information, assign to timeliness. 4. If the request is timeless and not tied to time or events, assign to time-independent. Only respond with the category name. Examples: definition of gravity, chess rules → time-independent; World War II, US president 1990 → explicit-time; Cannes festival 2025, French Revolution → event; Apple stock price, weather Lyon → timeliness\"\n",
    "#context = \"Categorize the query as one of the following and respond only with the category name: not-temporal (no direct or indirect time reference; relevance is static—e.g., definitions, rules, recipes), explicit-temporal (mentions a specific date, year, or defined time period—e.g., 'October 2024', 'last election'), or implicit-temporal (no explicit date, but relevance depends on current or recent context—e.g., weather, stock prices, travel, or buying intent, where up-to-date info or availability matters).\"\n",
    "QUERY_LIMIT = None  # Set to None to process all\n",
    "\n",
    "# Read CSV file\n",
    "with open(csv_file_path, mode='r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    fieldnames = reader.fieldnames + ['Answer']  # Add 'Answer' column\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "\n",
    "# Apply limit if set\n",
    "if QUERY_LIMIT:\n",
    "    rows = rows[:QUERY_LIMIT]\n",
    "\n",
    "# Process each row and send request to OpenAI API\n",
    "for row in rows:\n",
    "    prompt = row['query'] + context\n",
    "\n",
    "    # Payload for OpenAI API\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": max_length // 2  # Approximate token count for response\n",
    "    }\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Send request to OpenAI API\n",
    "    response = requests.post(api_url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        full_answer = response.json()['choices'][0]['message']['content'].strip()\n",
    "        row['Answer'] = full_answer[:max_length]\n",
    "    else:\n",
    "        row['Answer'] = \"Error: \" + response.text\n",
    "\n",
    "    if row_counter % 100 == 0:\n",
    "        print(f\"{row_counter} rows processed...\")\n",
    "\n",
    "    row_counter += 1\n",
    "\n",
    "with open(new_csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"Updated CSV file created at: {new_csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7832d4e6-0c28-486a-b635-c6de75f8ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Individual csv for test subcollections\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "combined_csv_path = \"unique_queries_2023-03_to_2023-08_updated.csv\"  # big CSV with all queries & categories\n",
    "specific_queries_txt = \"datasets/LongEval-Web/LongEval Test Collection/queries/2023-08_queries.txt\"  # the txt file for specific timeframe\n",
    "output_csv_path = \"2023-08_categorized_queries.csv\"     # output filtered CSV\n",
    "\n",
    "# Step 1: Load combined CSV\n",
    "combined_df = pd.read_csv(combined_csv_path)\n",
    "\n",
    "# Step 2: Load qids from the specific txt file\n",
    "with open(specific_queries_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "    qids_in_file = [line.split(\"\\t\")[0].strip() for line in f if line.strip()]\n",
    "\n",
    "# Step 3: Filter combined_df by qids present in txt file\n",
    "filtered_df = combined_df[combined_df[\"qid\"].astype(str).isin(qids_in_file)]\n",
    "\n",
    "# Step 4: Save filtered dataframe to new CSV\n",
    "filtered_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Filtered CSV saved with {len(filtered_df)} entries for {specific_queries_txt}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
