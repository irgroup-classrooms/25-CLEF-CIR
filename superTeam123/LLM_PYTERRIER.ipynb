{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3be2c3-b39d-4ede-813d-74bbc17269da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "\n",
    "# Database Configuration\n",
    "DATABASE = \"longeval-web\"\n",
    "USER = \"dis18\"\n",
    "HOST = \"db\"\n",
    "PORT = \"5432\"\n",
    "PASSWORD = \"dis182425\"\n",
    "\n",
    "# --- WICHTIG: Ersetzen Sie dies durch Ihren tatsächlichen OpenAI API Key ---\n",
    "# Sichern Sie Ihren API-Schlüssel, z.B. durch Umgebungsvariablen\n",
    "OPENAI_API_KEY = \"\" # Platzhalter\n",
    "if \"sk-proj-\" in OPENAI_API_KEY and len(OPENAI_API_KEY) > 70 : # Einfache Prüfung auf Platzhalterformat\n",
    "    print(\"WARNUNG: Bitte ersetzen Sie den Platzhalter OPENAI_API_KEY durch Ihren tatsächlichen Schlüssel in Zeile 15.\")\n",
    "    # exit() # Erwägen Sie, das Skript hier zu beenden, wenn der Schlüssel ein Platzhalter ist\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "# Output file configuration\n",
    "output_directory = \"Expanded\"\n",
    "output_filename = \"Query_Expansion_Final.csv\"\n",
    "output_file = os.path.join(output_directory, output_filename)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "\n",
    "def get_infos_from_openai(query_text):\n",
    "    \"\"\"\n",
    "    Ruft einen Zeitabhängigkeits-Score von OpenAI für den gegebenen Query-Text ab.\n",
    "    \"\"\"\n",
    "    prompt = f\"You have this Query. Give a score on how time dependant this Query:{query_text}.The Score is between 0 to 1. Don't answer with anything more than the Score.\"\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=10, # Score ist kurz, 10 Tokens sollten reichen\n",
    "            temperature=0  # Für deterministische Antworten\n",
    "        )\n",
    "        tag = response.choices[0].message.content.strip()\n",
    "        return tag\n",
    "    except Exception as e:\n",
    "        print(f\"      Fehler beim Aufruf der OpenAI API für Anfrage '{query_text}': {e}\")\n",
    "        return \"ERROR_API_CALL\" # Gibt ein eindeutiges Fehler-Tag zurück\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        engine = create_engine(f\"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")\n",
    "        \n",
    "        db_query = '''\n",
    "        SELECT DISTINCT \"text_fr\"\n",
    "        FROM \"Topic\";\n",
    "        '''\n",
    "        with engine.connect() as connection:\n",
    "            df_from_db = pd.read_sql(db_query, con=connection)\n",
    "        \n",
    "        print(f\"{len(df_from_db)} eindeutige Anfragen erfolgreich aus der Datenbank geladen.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Datenbankverbindung oder beim Abrufen der initialen Daten: {e}\")\n",
    "        return\n",
    "\n",
    "    processed_queries_set = set()\n",
    "    output_file_exists_and_valid = False\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        try:\n",
    "            # Versuche, die existierende Datei zu lesen\n",
    "            existing_df = pd.read_csv(output_file)\n",
    "            if not existing_df.empty and 'text_fr' in existing_df.columns:\n",
    "                # Stelle sicher, dass text_fr als String behandelt wird für den Set-Vergleich\n",
    "                processed_queries_set = set(existing_df['text_fr'].astype(str).tolist())\n",
    "                print(f\"{len(processed_queries_set)} bereits verarbeitete Anfragen aus '{output_file}' geladen.\")\n",
    "                output_file_exists_and_valid = True\n",
    "            else:\n",
    "                print(f\"'{output_file}' ist leer oder hat keine 'text_fr' Spalte. Wird als neue Datei behandelt.\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"'{output_file}' ist leer. Wird als neue Datei behandelt.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Lesen der existierenden CSV-Datei '{output_file}': {e}. Starte Verarbeitung neu für diese Datei.\")\n",
    "    \n",
    "    # Filtere die Datenbank-Anfragen, um nur neue zu bearbeiten\n",
    "    # Stelle sicher, dass text_fr aus der DB auch als String verglichen wird\n",
    "    df_from_db['text_fr'] = df_from_db['text_fr'].astype(str)\n",
    "    df_to_process = df_from_db[~df_from_db['text_fr'].isin(processed_queries_set)].copy()\n",
    "\n",
    "    num_new_queries = len(df_to_process)\n",
    "    num_skipped_queries = len(df_from_db) - num_new_queries\n",
    "\n",
    "    if num_skipped_queries > 0:\n",
    "        print(f\"{num_skipped_queries} Anfragen waren bereits verarbeitet und werden übersprungen.\")\n",
    "\n",
    "    if num_new_queries == 0:\n",
    "        print(\"Keine neuen Anfragen zu bearbeiten. Alle Datenbankeinträge sind bereits in der CSV-Datei vorhanden.\")\n",
    "        return\n",
    "\n",
    "    # Schreibe den Header, wenn die Datei nicht existierte oder nicht valide war\n",
    "    if not output_file_exists_and_valid:\n",
    "        try:\n",
    "            # 'w' Modus überschreibt die Datei oder erstellt sie neu\n",
    "            with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "                header_df = pd.DataFrame(columns=['text_fr', 'tag'])\n",
    "                header_df.to_csv(f, index=False)\n",
    "            print(f\"CSV-Header in '{output_file}' geschrieben (Datei neu erstellt oder war leer/inkonsistent).\")\n",
    "        except IOError as e:\n",
    "            print(f\"Fehler beim Schreiben des Headers in '{output_file}': {e}\")\n",
    "            return\n",
    "    \n",
    "    print(f\"\\nStarte OpenAI-Verarbeitung für {num_new_queries} neue Anfragen...\")\n",
    "\n",
    "    # Iteriere nur über die neuen/zu verarbeitenden Anfragen\n",
    "    for i, (_, row) in enumerate(df_to_process.iterrows()): # (_, row) da der Index von df_to_process nicht unbedingt bei 0 beginnt\n",
    "        current_query = row['text_fr']\n",
    "        \n",
    "        # Nummerierte Print-Ausgabe für den aktuellen Durchlauf\n",
    "        print(f\"Verarbeite neue Anfrage {i + 1}/{num_new_queries}: \\\"{current_query}\\\"\")\n",
    "        \n",
    "        tag = get_infos_from_openai(current_query)\n",
    "        \n",
    "        print(f\"  -> Tag erhalten: \\\"{tag}\\\"\")\n",
    "        \n",
    "        row_to_append = pd.DataFrame([{'text_fr': current_query, 'tag': tag}])\n",
    "        \n",
    "        # Hänge an die CSV-Datei an (header=False, da er schon existiert oder gerade geschrieben wurde)\n",
    "        try:\n",
    "            # 'a' Modus für Anfügen\n",
    "            with open(output_file, 'a', newline='', encoding='utf-8') as f:\n",
    "                row_to_append.to_csv(f, header=False, index=False)\n",
    "        except IOError as e:\n",
    "            print(f\"      Fehler beim Schreiben der Daten für Anfrage '{current_query}' in '{output_file}': {e}\")\n",
    "\n",
    "    print(f\"\\nVerarbeitung abgeschlossen. {num_new_queries} neue Anfragen wurden verarbeitet und in '{output_file}' gespeichert/angefügt.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed423f69-8f1f-4993-a108-d02941e41719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erfolgreich mit der Datenbank verbunden.\n",
      "47053 eindeutige Anfragen mit Tags aus 'Expanded/Query_Expansion_Final.csv' geladen.\n",
      "153891 Einträge aus der Tabelle 'Topic' geladen.\n",
      "\n",
      "Daten erfolgreich zusammengeführt.\n",
      "Das resultierende DataFrame hat 153891 Zeilen und 7 Spalten.\n",
      "\n",
      "Vorschau der zusammengeführten Daten (die ersten 5 Zeilen):\n",
      "                                     id queryid text_en  \\\n",
      "0  f72ba204-9502-4e2b-b5a3-886df465b726       3    None   \n",
      "1  238e44f2-87f9-49fb-93d2-70d983da668d       4    None   \n",
      "2  6e75448f-4832-4a59-acbf-e14722c3a48f       7    None   \n",
      "3  dd3e414c-78fd-4df9-a4fc-56d2ad8950b8       8    None   \n",
      "4  e25ebbf6-5250-4c29-b231-1dc1ede1683e       9    None   \n",
      "\n",
      "                    text_fr sub_collection split  tag  \n",
      "0      1ere guerre mondiale        2022-06  test  0.2  \n",
      "1  1ere guerre mondiale ce2        2022-06  test  0.9  \n",
      "2          3949 pole emploi        2022-06  test  0.5  \n",
      "3  4 mariages 1 enterrement        2022-06  test  0.2  \n",
      "4             4k video down        2022-06  test  0.2  \n",
      "\n",
      "Informationen zum zusammengeführten DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 153891 entries, 0 to 153890\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   id              153891 non-null  object \n",
      " 1   queryid         153891 non-null  object \n",
      " 2   text_en         0 non-null       object \n",
      " 3   text_fr         153891 non-null  object \n",
      " 4   sub_collection  153891 non-null  object \n",
      " 5   split           153891 non-null  object \n",
      " 6   tag             153891 non-null  float64\n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 8.2+ MB\n",
      "\n",
      "Alle Einträge aus der 'Topic'-Tabelle konnten erfolgreich mit Tags versehen werden.\n",
      "\n",
      "Die zusammengeführten Daten wurden erfolgreich in 'Expanded/Topics_With_TimeTags.csv' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "\n",
    "# Datenbankkonfiguration (wie in Ihren vorherigen Skripten)\n",
    "DATABASE = \"longeval-web\"\n",
    "USER = \"dis18\"\n",
    "HOST = \"db\"  # Stellen Sie sicher, dass 'db' vom Skriptumfeld aufgelöst werden kann\n",
    "PORT = \"5432\"\n",
    "PASSWORD = \"dis182425\"  # Vorsicht mit hartcodierten Passwörtern\n",
    "\n",
    "# Eingabedatei: Die CSV mit den eindeutigen text_fr und ihren Tags\n",
    "TAGGED_QUERIES_CSV = \"Expanded/Query_Expansion_Final.csv\"\n",
    "\n",
    "# Optionale Ausgabedatei für das zusammengeführte Ergebnis\n",
    "OUTPUT_MERGED_CSV = \"Expanded/Topics_With_TimeTags.csv\"\n",
    "\n",
    "def main():\n",
    "    # 1. Stelle eine Verbindung zur Datenbank her\n",
    "    try:\n",
    "        engine_string = f\"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\"\n",
    "        engine = create_engine(engine_string)\n",
    "        print(\"Erfolgreich mit der Datenbank verbunden.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Datenbankverbindung: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Lade die CSV-Datei mit den eindeutigen Anfragen und ihren Tags\n",
    "    if not os.path.exists(TAGGED_QUERIES_CSV):\n",
    "        print(f\"FEHLER: Die Datei mit den Tags '{TAGGED_QUERIES_CSV}' wurde nicht gefunden.\")\n",
    "        print(\"Bitte stellen Sie sicher, dass das vorherige Skript zur Tag-Erstellung erfolgreich ausgeführt wurde.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df_tags = pd.read_csv(TAGGED_QUERIES_CSV)\n",
    "        # Stelle sicher, dass 'text_fr' für ein konsistentes Merging als String behandelt wird\n",
    "        df_tags['text_fr'] = df_tags['text_fr'].astype(str)\n",
    "        # Entferne Duplikate in 'text_fr', falls wider Erwarten vorhanden (sollte schon eindeutig sein)\n",
    "        df_tags.drop_duplicates(subset=['text_fr'], inplace=True)\n",
    "        print(f\"{len(df_tags)} eindeutige Anfragen mit Tags aus '{TAGGED_QUERIES_CSV}' geladen.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden der Tag-Datei '{TAGGED_QUERIES_CSV}': {e}\")\n",
    "        return\n",
    "\n",
    "    # 3. Lade alle Daten aus der \"Topic\"-Tabelle (\"alles laden\")\n",
    "    # Annahme: \"alles laden\" bedeutet alle Spalten aus der Tabelle \"Topic\"\n",
    "    # Falls Sie eine spezifischere Auswahl benötigen, passen Sie die SQL-Query an.\n",
    "    query_all_topics = 'SELECT * FROM \"Topic\";'  # Lädt alle Spalten\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            df_all_topics = pd.read_sql(query_all_topics, con=connection)\n",
    "        # Stelle auch hier sicher, dass 'text_fr' für das Merging als String behandelt wird\n",
    "        if 'text_fr' in df_all_topics.columns:\n",
    "            df_all_topics['text_fr'] = df_all_topics['text_fr'].astype(str)\n",
    "        else:\n",
    "            print(\"FEHLER: Die Spalte 'text_fr' wurde nicht in der 'Topic'-Tabelle gefunden.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"{len(df_all_topics)} Einträge aus der Tabelle 'Topic' geladen.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden der Daten aus der Tabelle 'Topic': {e}\")\n",
    "        return\n",
    "\n",
    "    # 4. Führe die DataFrames zusammen\n",
    "    # Ein Left-Merge wird verwendet, um alle Zeilen aus df_all_topics beizubehalten\n",
    "    # und die 'tag'-Spalte aus df_tags hinzuzufügen, wo 'text_fr' übereinstimmt.\n",
    "    df_merged = pd.merge(df_all_topics, df_tags, on='text_fr', how='left')\n",
    "    \n",
    "    print(f\"\\nDaten erfolgreich zusammengeführt.\")\n",
    "    print(f\"Das resultierende DataFrame hat {len(df_merged)} Zeilen und {len(df_merged.columns)} Spalten.\")\n",
    "\n",
    "    # 5. Zeige einige Informationen und eine Vorschau der zusammengeführten Daten\n",
    "    print(\"\\nVorschau der zusammengeführten Daten (die ersten 5 Zeilen):\")\n",
    "    print(df_merged.head())\n",
    "\n",
    "    print(\"\\nInformationen zum zusammengeführten DataFrame:\")\n",
    "    df_merged.info()\n",
    "\n",
    "    # Überprüfe, ob es Einträge gab, denen kein Tag zugeordnet werden konnte\n",
    "    # Dies sollte idealerweise 0 sein, wenn alle 'text_fr' aus 'Topic' in der Tag-Datei vorhanden waren.\n",
    "    untagged_count = df_merged['tag'].isnull().sum()\n",
    "    if untagged_count > 0:\n",
    "        print(f\"\\nWARNUNG: {untagged_count} von {len(df_merged)} Einträgen in der 'Topic'-Tabelle konnten keinem Tag zugeordnet werden.\")\n",
    "        print(\"Dies kann passieren, wenn einige 'text_fr'-Werte aus der 'Topic'-Tabelle nicht in Ihrer '{TAGGED_QUERIES_CSV}'-Datei enthalten waren.\")\n",
    "        # print(\"Beispiele für nicht getaggte Anfragen (erste 5):\")\n",
    "        # print(df_merged[df_merged['tag'].isnull()].head())\n",
    "    else:\n",
    "        print(\"\\nAlle Einträge aus der 'Topic'-Tabelle konnten erfolgreich mit Tags versehen werden.\")\n",
    "\n",
    "    # 6. Optional: Speichere das zusammengeführte DataFrame in einer neuen CSV-Datei\n",
    "    try:\n",
    "        # Stelle sicher, dass das Ausgabeverzeichnis existiert\n",
    "        os.makedirs(os.path.dirname(OUTPUT_MERGED_CSV), exist_ok=True)\n",
    "        df_merged.to_csv(OUTPUT_MERGED_CSV, index=False)\n",
    "        print(f\"\\nDie zusammengeführten Daten wurden erfolgreich in '{OUTPUT_MERGED_CSV}' gespeichert.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nFehler beim Speichern der zusammengeführten Daten in '{OUTPUT_MERGED_CSV}': {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "413f9b54-8b5b-47a1-8aba-4dadad40e018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lese Datei: 'Expanded/Query_Expansion_Final.csv'...\n",
      "Bereinige die Spalte 'tag'...\n",
      "Bereinigung abgeschlossen. 1 Tag(s) wurden auf 0.0 gesetzt oder angepasst.\n",
      "\n",
      "Verteilung der Werte in der bereinigten Spalte 'tag':\n",
      "tag\n",
      "0.0      780\n",
      "0.1     8471\n",
      "0.2    23104\n",
      "0.3     3321\n",
      "0.5      716\n",
      "0.7     2859\n",
      "0.8     5819\n",
      "0.9     1410\n",
      "1.0      573\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Die bereinigten Daten wurden erfolgreich in 'Expanded/Query_Expansion_Final.csv' gespeichert (Original überschrieben).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Konfiguration ---\n",
    "# Bitte geben Sie den Pfad zu der CSV-Datei an, die Sie bereinigen möchten.\n",
    "# Zum Beispiel: \"Expanded/Query_Expansion_Final.csv\" oder \"Expanded/Topics_With_TimeTags.csv\"\n",
    "INPUT_CSV_FILE = \"Expanded/Query_Expansion_Final.csv\"\n",
    "# Die bereinigte Datei wird standardmäßig die Originaldatei überschreiben.\n",
    "# Wenn Sie eine neue Datei erstellen möchten, ändern Sie den folgenden Namen:\n",
    "OUTPUT_CSV_FILE = INPUT_CSV_FILE # Überschreibt die Originaldatei\n",
    "\n",
    "# Name der Spalte, die die Tags enthält\n",
    "TAG_COLUMN_NAME = 'tag'\n",
    "\n",
    "def clean_time_tag(tag_value):\n",
    "    \"\"\"\n",
    "    Bereinigt einen einzelnen Tag-Wert.\n",
    "    Gibt eine Zahl zwischen 0.0 und 1.0 zurück oder 0.0, wenn der Wert ungültig ist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Versuche, den Wert in eine Fließkommazahl umzuwandeln\n",
    "        numeric_tag = float(tag_value)\n",
    "        # Überprüfe, ob die Zahl im gültigen Bereich (0 bis 1) liegt\n",
    "        if 0.0 <= numeric_tag <= 1.0:\n",
    "            return numeric_tag\n",
    "        else:\n",
    "            # Zahl ist außerhalb des Bereichs, setze auf 0.0\n",
    "            return 0.0\n",
    "    except (ValueError, TypeError):\n",
    "        # Umwandlung fehlgeschlagen (z.B. Text wie \"ERROR_API_CALL\" oder der Wert ist None)\n",
    "        # Setze auf 0.0\n",
    "        return 0.0\n",
    "\n",
    "def main():\n",
    "    # Überprüfe, ob die Eingabedatei existiert\n",
    "    if not os.path.exists(INPUT_CSV_FILE):\n",
    "        print(f\"FEHLER: Die Eingabedatei '{INPUT_CSV_FILE}' wurde nicht gefunden.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Lese Datei: '{INPUT_CSV_FILE}'...\")\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV_FILE)\n",
    "    except Exception as e:\n",
    "        print(f\"FEHLER beim Lesen der CSV-Datei: {e}\")\n",
    "        return\n",
    "\n",
    "    # Überprüfe, ob die Tag-Spalte existiert\n",
    "    if TAG_COLUMN_NAME not in df.columns:\n",
    "        print(f\"FEHLER: Die Spalte '{TAG_COLUMN_NAME}' wurde in der Datei '{INPUT_CSV_FILE}' nicht gefunden.\")\n",
    "        print(f\"Vorhandene Spalten sind: {df.columns.tolist()}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Bereinige die Spalte '{TAG_COLUMN_NAME}'...\")\n",
    "\n",
    "    # Zähle, wie viele Werte geändert werden\n",
    "    original_tags = df[TAG_COLUMN_NAME].copy()\n",
    "    \n",
    "    # Wende die Bereinigungsfunktion auf jede Zelle in der Tag-Spalte an\n",
    "    df[TAG_COLUMN_NAME] = df[TAG_COLUMN_NAME].apply(clean_time_tag)\n",
    "\n",
    "    # Vergleiche die Originalwerte mit den neuen Werten, um die Anzahl der Änderungen zu ermitteln\n",
    "    # Beachte: Der Vergleich von Fließkommazahlen kann heikel sein, aber hier vergleichen wir\n",
    "    # das Ergebnis der Funktion (die 0.0 oder den Originalwert zurückgibt, wenn er gültig war)\n",
    "    # mit dem Original. Eine genauere Methode wäre, die Anzahl der Rückgaben von 0.0 zu zählen,\n",
    "    # wenn der Originalwert nicht 0.0 und gültig war.\n",
    "    # Einfacher: Zähle, wo sich der Wert geändert hat, nachdem sichergestellt wurde, dass beide numerisch sind für den Vergleich.\n",
    "    \n",
    "    # Um die Anzahl der tatsächlich geänderten Werte zu ermitteln (die nicht schon 0.0 waren oder gültig):\n",
    "    changed_count = 0\n",
    "    for original, new in zip(original_tags, df[TAG_COLUMN_NAME]):\n",
    "        # Prüfen, ob der neue Wert 0.0 ist UND der Originalwert nicht bereits ein gültiger Wert war, der zu 0.0 hätte werden können\n",
    "        # oder ob der Originalwert ein ungültiger String war.\n",
    "        try:\n",
    "            orig_float = float(original)\n",
    "            if not (0.0 <= orig_float <= 1.0) and new == 0.0: # War numerisch aber ungültig, wurde zu 0.0\n",
    "                 changed_count +=1\n",
    "            elif (0.0 <= orig_float <= 1.0) and orig_float != new : # War gültig, wurde aber trotzdem geändert (sollte nicht passieren mit clean_time_tag Logik)\n",
    "                 changed_count +=1 # Sicherheitscheck\n",
    "        except (ValueError, TypeError): # War nicht-numerisch (z.B. String) und wurde zu 0.0 (new == 0.0 ist implizit)\n",
    "            if new == 0.0: # Überprüfen, ob es tatsächlich zu 0.0 wurde\n",
    "                 changed_count +=1\n",
    "\n",
    "\n",
    "    print(f\"Bereinigung abgeschlossen. {changed_count} Tag(s) wurden auf 0.0 gesetzt oder angepasst.\")\n",
    "    \n",
    "    # Zähle die Verteilung der neuen Tags (optional, zur Information)\n",
    "    print(\"\\nVerteilung der Werte in der bereinigten Spalte '%s':\" % TAG_COLUMN_NAME)\n",
    "    print(df[TAG_COLUMN_NAME].value_counts(dropna=False).sort_index())\n",
    "\n",
    "\n",
    "    # Speichere das bereinigte DataFrame\n",
    "    try:\n",
    "        # Stelle sicher, dass das Ausgabeverzeichnis existiert, falls es Teil des Pfades ist\n",
    "        output_dir = os.path.dirname(OUTPUT_CSV_FILE)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        df.to_csv(OUTPUT_CSV_FILE, index=False)\n",
    "        if INPUT_CSV_FILE == OUTPUT_CSV_FILE:\n",
    "            print(f\"\\nDie bereinigten Daten wurden erfolgreich in '{OUTPUT_CSV_FILE}' gespeichert (Original überschrieben).\")\n",
    "        else:\n",
    "            print(f\"\\nDie bereinigten Daten wurden erfolgreich in '{OUTPUT_CSV_FILE}' gespeichert.\")\n",
    "    except Exception as e:\n",
    "        print(f\"FEHLER beim Speichern der bereinigten CSV-Datei: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9857406e-f69e-44d4-8d70-138baa8caff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
